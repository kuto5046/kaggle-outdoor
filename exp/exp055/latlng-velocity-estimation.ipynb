{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cddacd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:42.008810Z",
     "iopub.status.busy": "2021-07-11T12:07:42.007609Z",
     "iopub.status.idle": "2021-07-11T12:07:44.145305Z",
     "shell.execute_reply": "2021-07-11T12:07:44.144633Z",
     "shell.execute_reply.started": "2021-07-11T11:26:39.842648Z"
    },
    "papermill": {
     "duration": 2.169111,
     "end_time": "2021-07-11T12:07:44.145528",
     "exception": false,
     "start_time": "2021-07-11T12:07:41.976417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_square_error\n",
    "import random\n",
    "import math\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb394f9",
   "metadata": {
    "papermill": {
     "duration": 0.026719,
     "end_time": "2021-07-11T12:07:44.200781",
     "exception": false,
     "start_time": "2021-07-11T12:07:44.174062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 概要\n",
    "## 目的 \n",
    "- 緯度・経度速度を推定し、カルマンフィルタの観測に速度を追加することである。\n",
    "緯度・経度速度の定義は以下の通りである。\n",
    "$$\n",
    "LatVelocity(i) = \\frac{Lat(i)-Lat(i-1)}{time(i)-time(i-1)},\n",
    "LngVelocity(i) = \\frac{Lng(i)-Lng(i-1)}{time(i)-time(i-1)}\n",
    "$$\n",
    "\n",
    "## 結果\n",
    "### 速度推定\n",
    "- ぱっと見は良く推定できているように見える\n",
    "- ノイズ処理するだけでこれぐらいよくなる可能性はあるので、本質的に有効な情報かの判断は現状できない。\n",
    "- 推定速度の大きさに上限があるっぽいので、やはり早いところで使うと誤差の原因になりそう\n",
    "- SJCなどパルス的に精度の悪いことがある\n",
    "### 速度カルマンフィルタ\n",
    "- 0.05弱通常のカルマンフィルタより向上した（collectionでvalidationもしているのである程度効果はありそう）\n",
    " - Vanilla Kalman Filter:4.721716804543573\n",
    " - Velocity Kalman Filter:4.678559609622235\n",
    "- アンサンブルに関しては効果はほぼない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec752c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:44.261595Z",
     "iopub.status.busy": "2021-07-11T12:07:44.260900Z",
     "iopub.status.idle": "2021-07-11T12:07:44.264391Z",
     "shell.execute_reply": "2021-07-11T12:07:44.263748Z",
     "shell.execute_reply.started": "2021-07-11T11:26:41.906618Z"
    },
    "papermill": {
     "duration": 0.036606,
     "end_time": "2021-07-11T12:07:44.264532",
     "exception": false,
     "start_time": "2021-07-11T12:07:44.227926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_gt():\n",
    "    p = pathlib.Path(INPUT)\n",
    "    gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "    print('ground_truth.csv count : ', len(gt_files))\n",
    "\n",
    "    gts = []\n",
    "    for gt_file in tqdm(gt_files):\n",
    "        gts.append(pd.read_csv(gt_file))\n",
    "    ground_truth = pd.concat(gts)\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e566f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:44.325436Z",
     "iopub.status.busy": "2021-07-11T12:07:44.324750Z",
     "iopub.status.idle": "2021-07-11T12:07:46.017838Z",
     "shell.execute_reply": "2021-07-11T12:07:46.017175Z",
     "shell.execute_reply.started": "2021-07-11T11:26:41.913639Z"
    },
    "papermill": {
     "duration": 1.725934,
     "end_time": "2021-07-11T12:07:46.018026",
     "exception": false,
     "start_time": "2021-07-11T12:07:44.292092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT = '../../input/google-smartphone-decimeter-challenge'\n",
    "base_train = pd.read_csv(INPUT + '/' + 'baseline_locations_train.csv')\n",
    "base_test = pd.read_csv(INPUT + '/' + 'baseline_locations_test.csv')\n",
    "gt = make_gt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b037057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:46.094093Z",
     "iopub.status.busy": "2021-07-11T12:07:46.092948Z",
     "iopub.status.idle": "2021-07-11T12:07:46.096632Z",
     "shell.execute_reply": "2021-07-11T12:07:46.095918Z",
     "shell.execute_reply.started": "2021-07-11T11:26:43.535068Z"
    },
    "papermill": {
     "duration": 0.047319,
     "end_time": "2021-07-11T12:07:46.096799",
     "exception": false,
     "start_time": "2021-07-11T12:07:46.049480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_velocity(_df,col,shift=1,th=1e-1):\n",
    "    df = _df.copy()\n",
    "    df['out'] = (df[col]-df[col].shift(shift))/(df['millisSinceGpsEpoch']-df['millisSinceGpsEpoch'].shift(shift))\n",
    "    cond = (df['phoneName']!=df['phoneName'].shift(shift)) | (df['collectionName']!=df['collectionName'].shift(shift))\n",
    "    df.loc[cond,'out'] = 0\n",
    "    df['out'] = df['out']*1000#msからsのための変換\n",
    "    cond = df['out'].apply(lambda x:abs(x)) > th#以上データを除去するための処理\n",
    "    df.loc[cond,'out'] = 0\n",
    "    return df['out']\n",
    "\n",
    "def calc_shift(_df,col,shift=1):\n",
    "    df = _df.copy()\n",
    "    df['out'] = df[col].shift(shift)\n",
    "    cond = (df['phoneName']!=df['phoneName'].shift(shift)) | (df['collectionName']!=df['collectionName'].shift(shift))\n",
    "    df.loc[cond,'out'] = 0\n",
    "    return df['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2734b",
   "metadata": {
    "papermill": {
     "duration": 0.030954,
     "end_time": "2021-07-11T12:07:46.159091",
     "exception": false,
     "start_time": "2021-07-11T12:07:46.128137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 正解データ作り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc1d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:46.235308Z",
     "iopub.status.busy": "2021-07-11T12:07:46.233586Z",
     "iopub.status.idle": "2021-07-11T12:07:46.517955Z",
     "shell.execute_reply": "2021-07-11T12:07:46.517305Z",
     "shell.execute_reply.started": "2021-07-11T11:26:43.548406Z"
    },
    "papermill": {
     "duration": 0.326195,
     "end_time": "2021-07-11T12:07:46.518130",
     "exception": false,
     "start_time": "2021-07-11T12:07:46.191935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt['latv']=calc_velocity(gt,'latDeg')\n",
    "gt['lngv']=calc_velocity(gt,'lngDeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558db20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:46.606797Z",
     "iopub.status.busy": "2021-07-11T12:07:46.605931Z",
     "iopub.status.idle": "2021-07-11T12:07:47.221387Z",
     "shell.execute_reply": "2021-07-11T12:07:47.220718Z",
     "shell.execute_reply.started": "2021-07-11T11:26:43.800001Z"
    },
    "papermill": {
     "duration": 0.672057,
     "end_time": "2021-07-11T12:07:47.221535",
     "exception": false,
     "start_time": "2021-07-11T12:07:46.549478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(gt['latv'],bins=50,range=(-5e-4,5e-4))\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(gt['lngv'],bins=50,range=(-5e-4,5e-4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b417e9d",
   "metadata": {
    "papermill": {
     "duration": 0.032654,
     "end_time": "2021-07-11T12:07:47.295736",
     "exception": false,
     "start_time": "2021-07-11T12:07:47.263082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BaseLineから計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c0df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:47.364805Z",
     "iopub.status.busy": "2021-07-11T12:07:47.364105Z",
     "iopub.status.idle": "2021-07-11T12:07:47.610919Z",
     "shell.execute_reply": "2021-07-11T12:07:47.611426Z",
     "shell.execute_reply.started": "2021-07-11T11:26:44.370467Z"
    },
    "papermill": {
     "duration": 0.283952,
     "end_time": "2021-07-11T12:07:47.611611",
     "exception": false,
     "start_time": "2021-07-11T12:07:47.327659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_train['latv']=calc_velocity(base_train,'latDeg',th=5e-4)\n",
    "base_train['lngv']=calc_velocity(base_train,'lngDeg',th=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41999b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:47.688587Z",
     "iopub.status.busy": "2021-07-11T12:07:47.680823Z",
     "iopub.status.idle": "2021-07-11T12:07:48.112137Z",
     "shell.execute_reply": "2021-07-11T12:07:48.111544Z",
     "shell.execute_reply.started": "2021-07-11T11:26:44.489248Z"
    },
    "papermill": {
     "duration": 0.468802,
     "end_time": "2021-07-11T12:07:48.112310",
     "exception": false,
     "start_time": "2021-07-11T12:07:47.643508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(base_train['latv'],bins=50,range=(-5e-4,5e-4))\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(base_train['lngv'],bins=50,range=(-5e-4,5e-4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005ada7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:48.190449Z",
     "iopub.status.busy": "2021-07-11T12:07:48.187153Z",
     "iopub.status.idle": "2021-07-11T12:07:48.380448Z",
     "shell.execute_reply": "2021-07-11T12:07:48.379801Z",
     "shell.execute_reply.started": "2021-07-11T11:26:44.905463Z"
    },
    "papermill": {
     "duration": 0.233776,
     "end_time": "2021-07-11T12:07:48.380622",
     "exception": false,
     "start_time": "2021-07-11T12:07:48.146846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt = gt.rename(columns={'latv':'latv_truth','lngv':'lngv_truth'})\n",
    "base_train = pd.merge(base_train,gt.loc[:,['collectionName','phoneName','millisSinceGpsEpoch','latv_truth','lngv_truth']],\\\n",
    "                      on=['collectionName','phoneName','millisSinceGpsEpoch'],how='left')\n",
    "base_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec51aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:48.481634Z",
     "iopub.status.busy": "2021-07-11T12:07:48.480872Z",
     "iopub.status.idle": "2021-07-11T12:07:48.861878Z",
     "shell.execute_reply": "2021-07-11T12:07:48.862476Z",
     "shell.execute_reply.started": "2021-07-11T11:26:45.066570Z"
    },
    "papermill": {
     "duration": 0.447264,
     "end_time": "2021-07-11T12:07:48.862687",
     "exception": false,
     "start_time": "2021-07-11T12:07:48.415423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlim(-5e-4,5e-4)\n",
    "plt.ylim(-5e-4,5e-4)\n",
    "plt.scatter(base_train['latv'],base_train['latv_truth'],s=1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlim(-5e-4,5e-4)\n",
    "plt.ylim(-5e-4,5e-4)\n",
    "plt.scatter(base_train['lngv'],base_train['lngv_truth'],s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4da4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T10:59:16.498857Z",
     "iopub.status.busy": "2021-07-11T10:59:16.498193Z",
     "iopub.status.idle": "2021-07-11T10:59:16.503970Z",
     "shell.execute_reply": "2021-07-11T10:59:16.502745Z",
     "shell.execute_reply.started": "2021-07-11T10:59:16.498811Z"
    },
    "papermill": {
     "duration": 0.036435,
     "end_time": "2021-07-11T12:07:48.936183",
     "exception": false,
     "start_time": "2021-07-11T12:07:48.899748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBMで予想させる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e66cf",
   "metadata": {
    "papermill": {
     "duration": 0.035896,
     "end_time": "2021-07-11T12:07:49.010209",
     "exception": false,
     "start_time": "2021-07-11T12:07:48.974313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 特徴量生成（詳細説明略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eefbd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:49.094729Z",
     "iopub.status.busy": "2021-07-11T12:07:49.094029Z",
     "iopub.status.idle": "2021-07-11T12:07:57.491026Z",
     "shell.execute_reply": "2021-07-11T12:07:57.490256Z",
     "shell.execute_reply.started": "2021-07-11T11:26:45.473111Z"
    },
    "papermill": {
     "duration": 8.4448,
     "end_time": "2021-07-11T12:07:57.491228",
     "exception": false,
     "start_time": "2021-07-11T12:07:49.046428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in [1,2,4,8,16,32]:\n",
    "    base_train['dlatp'+str(i).zfill(2)]=calc_velocity(base_train,'latDeg',shift=i,th=1e-3)\n",
    "    base_train['dlngp'+str(i).zfill(2)]=calc_velocity(base_train,'lngDeg',shift=i,th=1e-3)\n",
    "    base_train['dlata'+str(i).zfill(2)]=calc_velocity(base_train,'latDeg',shift=(-i),th=1e-3)\n",
    "    base_train['dlnga'+str(i).zfill(2)]=calc_velocity(base_train,'lngDeg',shift=(-i),th=1e-3)\n",
    "    for j in range(1,2,4):\n",
    "        base_train['dlatp'+str(i).zfill(2)+'_shiftp'+str(j).zfill(2)] = calc_shift(base_train,'dlatp'+str(i).zfill(2),shift=j)\n",
    "        base_train['dlngp'+str(i).zfill(2)+'_shiftp'+str(j).zfill(2)] = calc_shift(base_train,'dlngp'+str(i).zfill(2),shift=j)\n",
    "        base_train['dlatp'+str(i).zfill(2)+'_shifta'+str(j).zfill(2)] = calc_shift(base_train,'dlatp'+str(i).zfill(2),shift=(-j))\n",
    "        base_train['dlngp'+str(i).zfill(2)+'_shifta'+str(j).zfill(2)] = calc_shift(base_train,'dlngp'+str(i).zfill(2),shift=(-j))\n",
    "        base_train['dlata'+str(i).zfill(2)+'_shiftp'+str(j).zfill(2)] = calc_shift(base_train,'dlata'+str(i).zfill(2),shift=j)\n",
    "        base_train['dlnga'+str(i).zfill(2)+'_shiftp'+str(j).zfill(2)] = calc_shift(base_train,'dlnga'+str(i).zfill(2),shift=j)\n",
    "        base_train['dlata'+str(i).zfill(2)+'_shifta'+str(j).zfill(2)] = calc_shift(base_train,'dlata'+str(i).zfill(2),shift=(-j))\n",
    "        base_train['dlnga'+str(i).zfill(2)+'_shifta'+str(j).zfill(2)] = calc_shift(base_train,'dlnga'+str(i).zfill(2),shift=(-j))\n",
    "print(len(base_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de01a1",
   "metadata": {
    "papermill": {
     "duration": 0.036434,
     "end_time": "2021-07-11T12:07:57.565207",
     "exception": false,
     "start_time": "2021-07-11T12:07:57.528773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0355c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:57.646946Z",
     "iopub.status.busy": "2021-07-11T12:07:57.646250Z",
     "iopub.status.idle": "2021-07-11T12:07:57.724988Z",
     "shell.execute_reply": "2021-07-11T12:07:57.725484Z",
     "shell.execute_reply.started": "2021-07-11T11:26:48.901071Z"
    },
    "papermill": {
     "duration": 0.123975,
     "end_time": "2021-07-11T12:07:57.725662",
     "exception": false,
     "start_time": "2021-07-11T12:07:57.601687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = base_train.copy()\n",
    "train_val_collection = list(base_train['collectionName'].unique())\n",
    "test_collection = random.sample(train_val_collection,14)\n",
    "train_val_collection = list(set(train_val_collection)-set(test_collection))\n",
    "print(len(train_val_collection),len(test_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bde238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:57.803952Z",
     "iopub.status.busy": "2021-07-11T12:07:57.802947Z",
     "iopub.status.idle": "2021-07-11T12:07:57.808944Z",
     "shell.execute_reply": "2021-07-11T12:07:57.808259Z",
     "shell.execute_reply.started": "2021-07-11T11:26:48.947802Z"
    },
    "papermill": {
     "duration": 0.045534,
     "end_time": "2021-07-11T12:07:57.809086",
     "exception": false,
     "start_time": "2021-07-11T12:07:57.763552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 過去のコンペのものを引っ張ってきただけ\n",
    "params = {\n",
    "        'task' : 'train',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'objective' : 'regression',\n",
    "        'metric' : {'l1'},\n",
    "        'num_leaves' : 11,\n",
    "        'learning_rate' : 0.1,\n",
    "        'feature_fraction' : 0.9,\n",
    "        'bagging_fraction' : 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose' : 0,\n",
    "        'early_stopping_rounds': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd0527",
   "metadata": {
    "papermill": {
     "duration": 0.036468,
     "end_time": "2021-07-11T12:07:57.883142",
     "exception": false,
     "start_time": "2021-07-11T12:07:57.846674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 緯度の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8e378",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-07-11T12:07:57.967386Z",
     "iopub.status.busy": "2021-07-11T12:07:57.966715Z",
     "iopub.status.idle": "2021-07-11T12:09:11.305615Z",
     "shell.execute_reply": "2021-07-11T12:09:11.306617Z",
     "shell.execute_reply.started": "2021-07-11T11:26:48.954691Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 73.387193,
     "end_time": "2021-07-11T12:09:11.306818",
     "exception": false,
     "start_time": "2021-07-11T12:07:57.919625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "test = data[data['collectionName'].isin(test_collection)]\n",
    "testX = test.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_collection = random.sample(train_val_collection,10)\n",
    "    val_collection = list(set(train_val_collection)-set(train_collection))\n",
    "    train = data[data['collectionName'].isin(train_collection)]\n",
    "    val = data[data['collectionName'].isin(val_collection)]\n",
    "    trainX = train.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "    trainY = train['latv_truth']\n",
    "    valX = val.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "    valY = val['latv_truth']\n",
    "    \n",
    "    lgb_train = lgb.Dataset(trainX, label=trainY)\n",
    "    lgb_val = lgb.Dataset(valX, label=valY, reference= lgb_train)\n",
    "    gbm = lgb.train(params,lgb_train,valid_sets=lgb_val,num_boost_round=100000,verbose_eval=100)\n",
    "    if e==0:\n",
    "        test['latv_pred'] = gbm.predict(testX)/epoch\n",
    "    else:\n",
    "        test['latv_pred'] += gbm.predict(testX)/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed912fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:09:11.473965Z",
     "iopub.status.busy": "2021-07-11T12:09:11.473273Z",
     "iopub.status.idle": "2021-07-11T12:11:18.820840Z",
     "shell.execute_reply": "2021-07-11T12:11:18.821508Z",
     "shell.execute_reply.started": "2021-07-11T11:28:35.293025Z"
    },
    "papermill": {
     "duration": 127.441197,
     "end_time": "2021-07-11T12:11:18.821806",
     "exception": false,
     "start_time": "2021-07-11T12:09:11.380609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#epoch = 5\n",
    "#test = data[data['collectionName'].isin(test_collection)]\n",
    "#testX = test.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_collection = random.sample(train_val_collection,10)\n",
    "    val_collection = list(set(train_val_collection)-set(train_collection))\n",
    "    train = data[data['collectionName'].isin(train_collection)]\n",
    "    val = data[data['collectionName'].isin(val_collection)]\n",
    "    trainX = train.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "    trainY = train['lngv_truth']\n",
    "    valX = val.drop(columns=['collectionName','phoneName','latDeg','lngDeg','millisSinceGpsEpoch','heightAboveWgs84EllipsoidM','phone','latv_truth','lngv_truth'])\n",
    "    valY = val['lngv_truth']\n",
    "    \n",
    "    lgb_train = lgb.Dataset(trainX, label=trainY)\n",
    "    lgb_val = lgb.Dataset(valX, label=valY, reference= lgb_train)\n",
    "    gbm = lgb.train(params,lgb_train,valid_sets=lgb_val,num_boost_round=100000,verbose_eval=100)\n",
    "    if e==0:\n",
    "        test['lngv_pred'] = gbm.predict(testX)/epoch\n",
    "    else:\n",
    "        test['lngv_pred'] += gbm.predict(testX)/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ef96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:11:19.116593Z",
     "iopub.status.busy": "2021-07-11T12:11:19.115882Z",
     "iopub.status.idle": "2021-07-11T12:11:19.406485Z",
     "shell.execute_reply": "2021-07-11T12:11:19.405867Z",
     "shell.execute_reply.started": "2021-07-11T11:30:45.663268Z"
    },
    "papermill": {
     "duration": 0.448284,
     "end_time": "2021-07-11T12:11:19.406627",
     "exception": false,
     "start_time": "2021-07-11T12:11:18.958343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlim(-5e-4,5e-4)\n",
    "plt.ylim(-5e-4,5e-4)\n",
    "plt.scatter(test['latv_truth'],test['latv_pred'],s=1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlim(-5e-4,5e-4)\n",
    "plt.ylim(-5e-4,5e-4)\n",
    "plt.scatter(test['lngv_truth'],test['lngv_pred'],s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dea32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:11:19.683945Z",
     "iopub.status.busy": "2021-07-11T12:11:19.683225Z",
     "iopub.status.idle": "2021-07-11T12:11:26.232456Z",
     "shell.execute_reply": "2021-07-11T12:11:26.231813Z",
     "shell.execute_reply.started": "2021-07-11T11:30:45.927911Z"
    },
    "papermill": {
     "duration": 6.692101,
     "end_time": "2021-07-11T12:11:26.232612",
     "exception": false,
     "start_time": "2021-07-11T12:11:19.540511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c,d in test.groupby(['collectionName','phoneName']):\n",
    "    print(c)\n",
    "    plt.figure(figsize=(25,3))\n",
    "    plt.plot(d['latv_pred'])\n",
    "    plt.plot(d['latv_truth'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31c092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T11:20:38.293493Z",
     "iopub.status.busy": "2021-07-11T11:20:38.293158Z",
     "iopub.status.idle": "2021-07-11T11:20:38.299508Z",
     "shell.execute_reply": "2021-07-11T11:20:38.297914Z",
     "shell.execute_reply.started": "2021-07-11T11:20:38.293464Z"
    },
    "papermill": {
     "duration": 0.218725,
     "end_time": "2021-07-11T12:11:26.673868",
     "exception": false,
     "start_time": "2021-07-11T12:11:26.455143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ここまでの考察\n",
    "- ぱっと見は良く推定できているように見える\n",
    "- ノイズ処理するだけでこれぐらいよくなる可能性はあるので、本質的に有効な情報かの判断は現状できない。\n",
    "- 推定速度の大きさに上限があるっぽいので、やはり早いところで使うと誤差の原因になりそう\n",
    "- SJCはやや精度が悪い？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205aee0b",
   "metadata": {
    "papermill": {
     "duration": 0.218982,
     "end_time": "2021-07-11T12:11:27.110091",
     "exception": false,
     "start_time": "2021-07-11T12:11:26.891109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vanilla Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96734be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:11:27.568352Z",
     "iopub.status.busy": "2021-07-11T12:11:27.550874Z",
     "iopub.status.idle": "2021-07-11T12:11:36.903474Z",
     "shell.execute_reply": "2021-07-11T12:11:36.902802Z",
     "shell.execute_reply.started": "2021-07-11T11:34:17.441758Z"
    },
    "papermill": {
     "duration": 9.576152,
     "end_time": "2021-07-11T12:11:36.903671",
     "exception": false,
     "start_time": "2021-07-11T12:11:27.327519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install simdkalman\n",
    "import simdkalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a19c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:11:37.377078Z",
     "iopub.status.busy": "2021-07-11T12:11:37.366048Z",
     "iopub.status.idle": "2021-07-11T12:11:37.444493Z",
     "shell.execute_reply": "2021-07-11T12:11:37.443792Z",
     "shell.execute_reply.started": "2021-07-11T11:50:28.031868Z"
    },
    "papermill": {
     "duration": 0.317251,
     "end_time": "2021-07-11T12:11:37.444677",
     "exception": false,
     "start_time": "2021-07-11T12:11:37.127426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist\n",
    "\n",
    "def make_gt(recal):\n",
    "    if recal:\n",
    "        p = pathlib.Path(INPUT)\n",
    "        gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "        print('ground_truth.csv count : ', len(gt_files))\n",
    "\n",
    "        gts = []\n",
    "        for gt_file in tqdm(gt_files):\n",
    "            gts.append(pd.read_csv(gt_file))\n",
    "        ground_truth = pd.concat(gts)\n",
    "        #ground_truth.to_csv('./base/gt.csv',index=False)\n",
    "    else:\n",
    "        ground_truth = pd.read_csv('./base/gt.csv')\n",
    "    return ground_truth\n",
    "\n",
    "def add_distance_diff(df):\n",
    "    df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "    df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "    df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "    df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "    df['phone_prev'] = df['phone'].shift(1)\n",
    "    df['phone_next'] = df['phone'].shift(-1)\n",
    "    \n",
    "    df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "    df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev']] = np.nan\n",
    "    df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next']] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def check_time(_df):\n",
    "    data = _df.copy().reset_index(drop=True)\n",
    "    data['ms_dif'] = data['millisSinceGpsEpoch']-data['millisSinceGpsEpoch'].shift()\n",
    "    mis_time = (len(data)-1-data['ms_dif'].value_counts()[1000.0])/len(data)\n",
    "    return mis_time>0.0\n",
    "\n",
    "def adjust_time(df):\n",
    "    data = df.copy().reset_index(drop=True)\n",
    "    new_data = data.copy()\n",
    "    t = 'millisSinceGpsEpoch'\n",
    "    start = data['millisSinceGpsEpoch'].values[0]\n",
    "    end = data['millisSinceGpsEpoch'].values[-1]\n",
    "    ind = math.ceil((end-start)/1000)\n",
    "    _data = pd.DataFrame(index=range(ind))\n",
    "    _data['latDeg']=0\n",
    "    _data['lngDeg']=0\n",
    "    #_data['millisSinceGpsEpoch'] = 0\n",
    "    _data['millisSinceGpsEpoch'] = start+1000*_data.index\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in _data.index:\n",
    "        while _data.loc[i,t]<new_data.loc[cnt,t] or _data.loc[i,t]>new_data.loc[cnt+1,t]:\n",
    "            cnt+=1\n",
    "        if _data.loc[i,t]==new_data.loc[cnt,t]:\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']\n",
    "        else:\n",
    "            w=(new_data.loc[cnt+1,t]-_data.loc[i,t])/(new_data.loc[cnt+1,t]-new_data.loc[cnt,t])\n",
    "            if w<0 or w>1:\n",
    "                print(w)\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']*w+new_data.loc[cnt+1,'latDeg']*(1-w)\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']*w+new_data.loc[cnt+1,'lngDeg']*(1-w)\n",
    "    _data.head()\n",
    "    __data = _data.loc[:,['latDeg', 'lngDeg']].to_numpy()\n",
    "    __data = __data.reshape(1, len(__data), 2)\n",
    "    smoothed = kf.smooth(__data)\n",
    "    _data.loc[:,'latDeg_pred'] = smoothed.states.mean[0, :, 0]\n",
    "    _data.loc[:,'lngDeg_pred'] = smoothed.states.mean[0, :, 1]\n",
    "    _data.loc[:,'latDeg_cov_pred'] = smoothed.states.cov[0, :, 0,0]\n",
    "    _data.loc[:,'lngDeg_cov_pred'] = smoothed.states.cov[0, :, 1,1]\n",
    "    data['latDeg_cov'] = 1\n",
    "    data['lngDeg_cov'] = 1\n",
    "\n",
    "    cnt = 0\n",
    "    for i in data.index:\n",
    "        try:\n",
    "            while data.loc[i,t]<_data.loc[cnt,t] or data.loc[i,t]>_data.loc[cnt+1,t]:\n",
    "                cnt+=1\n",
    "            if data.loc[i,t]==_data.loc[cnt,t]:\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']\n",
    "            else:\n",
    "                w=(_data.loc[cnt+1,t]-data.loc[i,t])/(_data.loc[cnt+1,t]-_data.loc[cnt,t])\n",
    "                if w<0 or w>1:\n",
    "                    print(w)\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']*w+_data.loc[cnt+1,'latDeg_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']*w+_data.loc[cnt+1,'lngDeg_pred']*(1-w)\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']*w+_data.loc[cnt+1,'latDeg_cov_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']*w+_data.loc[cnt+1,'lngDeg_cov_pred']*(1-w)\n",
    "        except:\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kf):\n",
    "    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n",
    "    for collection, phone in unique_paths:\n",
    "        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n",
    "        if check_time(df[cond]):\n",
    "            _df = adjust_time(df[cond])\n",
    "            df.loc[cond, 'latDeg'] = _df['latDeg'].values\n",
    "            df.loc[cond, 'lngDeg'] = _df['lngDeg'].values\n",
    "            df.loc[cond, 'latDeg_cov'] = _df['latDeg_cov'].values\n",
    "            df.loc[cond, 'lngDeg_cov'] = _df['lngDeg_cov'].values\n",
    "        else:\n",
    "            data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n",
    "            data = data.reshape(1, len(data), 2)\n",
    "            smoothed = kf_.smooth(data)\n",
    "            df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "            #print(smoothed.states.mean.shape,smoothed.states.cov.shape)\n",
    "            df.loc[cond, 'latDeg_cov'] = smoothed.states.cov[0, :, 0,0]\n",
    "            df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "            df.loc[cond, 'lngDeg_cov'] = smoothed.states.cov[0, :, 1,1]\n",
    "    return df\n",
    "\n",
    "def weighted_make_lerp_data(df):\n",
    "    '''\n",
    "    Generate interpolated lat,lng values for different phone times in the same collection.\n",
    "    '''\n",
    "    org_columns = df.columns\n",
    "    \n",
    "    # Generate a combination of time x collection x phone and combine it with the original data (generate records to be interpolated)\n",
    "    time_list = df[['collectionName', 'millisSinceGpsEpoch']].drop_duplicates()\n",
    "    phone_list =df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "    tmp = time_list.merge(phone_list, on='collectionName', how='outer')\n",
    "    \n",
    "    lerp_df = tmp.merge(df, on=['collectionName', 'millisSinceGpsEpoch', 'phoneName'], how='left')\n",
    "    lerp_df['phone'] = lerp_df['collectionName'] + '_' + lerp_df['phoneName']\n",
    "    lerp_df = lerp_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n",
    "    \n",
    "    # linear interpolation\n",
    "    lerp_df['latDeg_prev'] = lerp_df['latDeg'].shift(1)\n",
    "    lerp_df['latDeg_next'] = lerp_df['latDeg'].shift(-1)\n",
    "    lerp_df['lngDeg_prev'] = lerp_df['lngDeg'].shift(1)\n",
    "    lerp_df['lngDeg_next'] = lerp_df['lngDeg'].shift(-1)\n",
    "    lerp_df['phone_prev'] = lerp_df['phone'].shift(1)\n",
    "    lerp_df['phone_next'] = lerp_df['phone'].shift(-1)\n",
    "    lerp_df['time_prev'] = lerp_df['millisSinceGpsEpoch'].shift(1)\n",
    "    lerp_df['time_next'] = lerp_df['millisSinceGpsEpoch'].shift(-1)\n",
    "    lerp_df['latDeg_cov_prev'] = lerp_df['latDeg_cov'].shift(1)\n",
    "    lerp_df['latDeg_cov_next'] = lerp_df['latDeg_cov'].shift(-1)\n",
    "    lerp_df['lngDeg_cov_prev'] = lerp_df['lngDeg_cov'].shift(1)\n",
    "    lerp_df['lngDeg_cov_next'] = lerp_df['lngDeg_cov'].shift(-1)\n",
    "    # Leave only records to be interpolated\n",
    "    lerp_df = lerp_df[(lerp_df['latDeg'].isnull())&(lerp_df['phone']==lerp_df['phone_prev'])&(lerp_df['phone']==lerp_df['phone_next'])].copy()\n",
    "    # calc lerp\n",
    "    lerp_df['latDeg'] = lerp_df['latDeg_prev'] + ((lerp_df['latDeg_next'] - lerp_df['latDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    lerp_df['lngDeg'] = lerp_df['lngDeg_prev'] + ((lerp_df['lngDeg_next'] - lerp_df['lngDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    lerp_df['latDeg_cov'] = lerp_df['latDeg_cov_prev'] + ((lerp_df['latDeg_cov_next'] - lerp_df['latDeg_cov_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    lerp_df['lngDeg_cov'] = lerp_df['lngDeg_cov_prev'] + ((lerp_df['lngDeg_cov_next'] - lerp_df['lngDeg_cov_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    \n",
    "    # Leave only the data that has a complete set of previous and next data.\n",
    "    lerp_df = lerp_df[~lerp_df['latDeg'].isnull()]\n",
    "    \n",
    "    return lerp_df[org_columns]\n",
    "\n",
    "def weighted_calc_mean_pred(df, lerp_df):\n",
    "    '''\n",
    "    Make a prediction based on the average of the predictions of phones in the same collection.\n",
    "    '''\n",
    "    cnt = 0\n",
    "    add_lerp = pd.concat([df, lerp_df])\n",
    "    #mean_pred_result = add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])[['latDeg', 'lngDeg']].mean().reset_index()\n",
    "    mean_pred_result = []\n",
    "    for i,(c,_df) in enumerate(add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])):\n",
    "        #print(c,len(df))\n",
    "        if len(_df)>1:\n",
    "            wsum_lat = 0\n",
    "            wsum_lng = 0\n",
    "            for j in range(len(_df)):\n",
    "                if j==0:\n",
    "                    _df.loc[_df.index[0],'latDeg'] = _df.loc[_df.index[j],'latDeg']/np.power(_df.loc[_df.index[j],'latDeg_cov'],8)\n",
    "                    _df.loc[_df.index[0],'lngDeg'] = _df.loc[_df.index[j],'lngDeg']/np.power(_df.loc[_df.index[j],'lngDeg_cov'],8)\n",
    "                else:\n",
    "                    _df.loc[_df.index[0],'latDeg'] += _df.loc[_df.index[j],'latDeg']/np.power(_df.loc[_df.index[j],'latDeg_cov'],8)\n",
    "                    _df.loc[_df.index[0],'lngDeg'] += _df.loc[_df.index[j],'lngDeg']/np.power(_df.loc[_df.index[j],'lngDeg_cov'],8)\n",
    "                wsum_lat += 1/np.power(_df.loc[_df.index[j],'latDeg_cov'],8)\n",
    "                wsum_lng += 1/np.power(_df.loc[_df.index[j],'lngDeg_cov'],8)\n",
    "            _df.loc[_df.index[0],'latDeg'] = _df.loc[_df.index[0],'latDeg']/wsum_lat\n",
    "            _df.loc[_df.index[0],'lngDeg'] = _df.loc[_df.index[0],'lngDeg']/wsum_lng\n",
    "            _df = _df.loc[[_df.index[0]],:]\n",
    "            #_df.iloc[0,3] = (_df.iloc[0,3]*_df.iloc[1,5]*_df.iloc[1,5]+_df.iloc[1,3]*_df.iloc[0,5]*_df.iloc[0,5])/(_df.iloc[0,5]*_df.iloc[0,5]+_df.iloc[1,5]*_df.iloc[1,5])\n",
    "            #_df.iloc[0,4] = (_df.iloc[0,4]*_df.iloc[1,6]*_df.iloc[1,6]+_df.iloc[1,4]*_df.iloc[0,6]*_df.iloc[0,6])/(_df.iloc[0,6]*_df.iloc[0,6]+_df.iloc[1,6]*_df.iloc[1,6])\n",
    "            #_df = _df.iloc[[0],:]\n",
    "        mean_pred_result.append(_df)\n",
    "    mean_pred_result = pd.concat(mean_pred_result)  \n",
    "    mean_pred_df = df[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].copy()\n",
    "    mean_pred_df = mean_pred_df.merge(mean_pred_result[['collectionName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=['collectionName', 'millisSinceGpsEpoch'], how='left')\n",
    "    return mean_pred_df\n",
    "\n",
    "def percentile50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def percentile95(x):\n",
    "    return np.percentile(x, 95)\n",
    "\n",
    "def get_train_score(df, gt):\n",
    "    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "    df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='left')\n",
    "    # calc_distance_error\n",
    "    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n",
    "    # calc_evaluate_score\n",
    "    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    res = df.groupby('phone')['err'].agg([percentile50, percentile95])\n",
    "    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2 \n",
    "    score = res['p50_p90_mean'].mean()\n",
    "    return score\n",
    "\n",
    "def get_removedevice(input_df: pd.DataFrame, divece: str) -> pd.DataFrame:\n",
    "    input_df['index'] = input_df.index\n",
    "    input_df = input_df.sort_values('millisSinceGpsEpoch')\n",
    "    input_df.index = input_df['millisSinceGpsEpoch'].values\n",
    "\n",
    "    output_df = pd.DataFrame() \n",
    "    for _, subdf in input_df.groupby('collectionName'):\n",
    "\n",
    "        phones = subdf['phoneName'].unique()\n",
    "\n",
    "        if (len(phones) == 1) or (not divece in phones):\n",
    "            output_df = pd.concat([output_df, subdf])\n",
    "            continue\n",
    "\n",
    "        origin_df = subdf.copy()\n",
    "        \n",
    "        _index = subdf['phoneName']==divece\n",
    "        subdf.loc[_index, 'latDeg'] = np.nan\n",
    "        subdf.loc[_index, 'lngDeg'] = np.nan\n",
    "        subdf = subdf.interpolate(method='index', limit_area='inside')\n",
    "\n",
    "        _index = subdf['latDeg'].isnull()\n",
    "        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n",
    "        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n",
    "\n",
    "        output_df = pd.concat([output_df, subdf])\n",
    "\n",
    "    output_df.index = output_df['index'].values\n",
    "    output_df = output_df.sort_index()\n",
    "\n",
    "    del output_df['index']\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def get_remove_data(input_df: pd.DataFrame, collection_device) -> pd.DataFrame:\n",
    "    input_df['index'] = input_df.index\n",
    "    input_df = input_df.sort_values('millisSinceGpsEpoch')\n",
    "    input_df.index = input_df['millisSinceGpsEpoch'].values\n",
    "\n",
    "    output_df = pd.DataFrame() \n",
    "    for c, subdf in input_df.groupby('collectionName'):\n",
    "\n",
    "        phones = subdf['phoneName'].unique()\n",
    "\n",
    "        devices = [x[1] for x in collection_device if x[0]==c]\n",
    "        if (len(phones) == 1) or (len(devices)==0):\n",
    "            output_df = pd.concat([output_df, subdf])\n",
    "            continue\n",
    "\n",
    "        origin_df = subdf.copy()\n",
    "        _index = subdf['phoneName'].isin(devices)\n",
    "        subdf.loc[_index, 'latDeg'] = np.nan\n",
    "        subdf.loc[_index, 'lngDeg'] = np.nan\n",
    "        subdf = subdf.interpolate(method='index', limit_area='inside')\n",
    "\n",
    "        _index = subdf['latDeg'].isnull()\n",
    "        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n",
    "        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n",
    "\n",
    "        output_df = pd.concat([output_df, subdf])\n",
    "\n",
    "    output_df.index = output_df['index'].values\n",
    "    output_df = output_df.sort_index()\n",
    "\n",
    "    del output_df['index']\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a697b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:11:37.901124Z",
     "iopub.status.busy": "2021-07-11T12:11:37.900394Z",
     "iopub.status.idle": "2021-07-11T12:13:05.416830Z",
     "shell.execute_reply": "2021-07-11T12:13:05.417641Z",
     "shell.execute_reply.started": "2021-07-11T11:50:46.686636Z"
    },
    "papermill": {
     "duration": 87.749773,
     "end_time": "2021-07-11T12:13:05.417888",
     "exception": false,
     "start_time": "2021-07-11T12:11:37.668115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ro = add_distance_diff(test).copy()\n",
    "th = 50\n",
    "print('outlier filtering')\n",
    "test_ro.loc[((test_ro['dist_prev'] > th) & (test_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n",
    "print('appling kf smoothing')\n",
    "test_ro_kf = apply_kf_smoothing(test_ro[cols])\n",
    "vanilla_kal = test_ro_kf.copy()\n",
    "print(get_train_score(test,gt))\n",
    "print(get_train_score(test_ro_kf,gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260cddb",
   "metadata": {
    "papermill": {
     "duration": 0.219904,
     "end_time": "2021-07-11T12:13:05.859430",
     "exception": false,
     "start_time": "2021-07-11T12:13:05.639526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Velocity Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b37c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:13:06.308962Z",
     "iopub.status.busy": "2021-07-11T12:13:06.308205Z",
     "iopub.status.idle": "2021-07-11T12:13:06.346930Z",
     "shell.execute_reply": "2021-07-11T12:13:06.347511Z",
     "shell.execute_reply.started": "2021-07-11T11:43:37.115552Z"
    },
    "papermill": {
     "duration": 0.265528,
     "end_time": "2021-07-11T12:13:06.347699",
     "exception": false,
     "start_time": "2021-07-11T12:13:06.082171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0],[0,0,1,0,0,0],[0,0,0,1,0,0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5, 5e-6,5e-6]) + np.ones((4, 4)) * 1e-9\n",
    "\n",
    "kfv = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def adjust_time(df):\n",
    "    data = df.copy().reset_index(drop=True)\n",
    "    new_data = data.copy()\n",
    "    t = 'millisSinceGpsEpoch'\n",
    "    start = data['millisSinceGpsEpoch'].values[0]\n",
    "    end = data['millisSinceGpsEpoch'].values[-1]\n",
    "    ind = math.ceil((end-start)/1000)\n",
    "    _data = pd.DataFrame(index=range(ind))\n",
    "    _data['latDeg']=0\n",
    "    _data['lngDeg']=0\n",
    "    _data['latv_pred']=0\n",
    "    _data['lngv_pred']=0\n",
    "    #_data['millisSinceGpsEpoch'] = 0\n",
    "    _data['millisSinceGpsEpoch'] = start+1000*_data.index\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in _data.index:\n",
    "        while _data.loc[i,t]<new_data.loc[cnt,t] or _data.loc[i,t]>new_data.loc[cnt+1,t]:\n",
    "            cnt+=1\n",
    "        if _data.loc[i,t]==new_data.loc[cnt,t]:\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']\n",
    "            _data.loc[i,'latv_pred'] = new_data.loc[cnt,'latv_pred']\n",
    "            _data.loc[i,'lngv_pred'] = new_data.loc[cnt,'lngv_pred']\n",
    "        else:\n",
    "            w=(new_data.loc[cnt+1,t]-_data.loc[i,t])/(new_data.loc[cnt+1,t]-new_data.loc[cnt,t])\n",
    "            if w<0 or w>1:\n",
    "                print(w)\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']*w+new_data.loc[cnt+1,'latDeg']*(1-w)\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']*w+new_data.loc[cnt+1,'lngDeg']*(1-w)\n",
    "            _data.loc[i,'latv_pred'] = new_data.loc[cnt,'latv_pred']*w+new_data.loc[cnt+1,'latv_pred']*(1-w)\n",
    "            _data.loc[i,'lngv_pred'] = new_data.loc[cnt,'lngv_pred']*w+new_data.loc[cnt+1,'lngv_pred']*(1-w)\n",
    "    _data.head()\n",
    "    __data = _data.loc[:,['latDeg', 'lngDeg','latv_pred','lngv_pred']].to_numpy()\n",
    "    __data = __data.reshape(1, len(__data), 4)\n",
    "    smoothed = kfv.smooth(__data)\n",
    "    _data.loc[:,'latDeg_pred'] = smoothed.states.mean[0, :, 0]\n",
    "    _data.loc[:,'lngDeg_pred'] = smoothed.states.mean[0, :, 1]\n",
    "    _data.loc[:,'latDeg_cov_pred'] = smoothed.states.cov[0, :, 0,0]\n",
    "    _data.loc[:,'lngDeg_cov_pred'] = smoothed.states.cov[0, :, 1,1]\n",
    "    data['latDeg_cov'] = 1\n",
    "    data['lngDeg_cov'] = 1\n",
    "\n",
    "    cnt = 0\n",
    "    for i in data.index:\n",
    "        try:\n",
    "            while data.loc[i,t]<_data.loc[cnt,t] or data.loc[i,t]>_data.loc[cnt+1,t]:\n",
    "                cnt+=1\n",
    "            if data.loc[i,t]==_data.loc[cnt,t]:\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']\n",
    "            else:\n",
    "                w=(_data.loc[cnt+1,t]-data.loc[i,t])/(_data.loc[cnt+1,t]-_data.loc[cnt,t])\n",
    "                if w<0 or w>1:\n",
    "                    print(w)\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']*w+_data.loc[cnt+1,'latDeg_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']*w+_data.loc[cnt+1,'lngDeg_pred']*(1-w)\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']*w+_data.loc[cnt+1,'latDeg_cov_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']*w+_data.loc[cnt+1,'lngDeg_cov_pred']*(1-w)\n",
    "        except:\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kfv):\n",
    "    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n",
    "    for collection, phone in unique_paths:\n",
    "        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n",
    "        if check_time(df[cond]):\n",
    "            _df = adjust_time(df[cond])\n",
    "            df.loc[cond, 'latDeg'] = _df['latDeg'].values\n",
    "            df.loc[cond, 'lngDeg'] = _df['lngDeg'].values\n",
    "            df.loc[cond, 'latDeg_cov'] = _df['latDeg_cov'].values\n",
    "            df.loc[cond, 'lngDeg_cov'] = _df['lngDeg_cov'].values\n",
    "        else:\n",
    "            data = df[cond][['latDeg', 'lngDeg','latv_pred','lngv_pred']].to_numpy()\n",
    "            data = data.reshape(1, len(data), 4)\n",
    "            smoothed = kf_.smooth(data)\n",
    "            df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "            #print(smoothed.states.mean.shape,smoothed.states.cov.shape)\n",
    "            df.loc[cond, 'latDeg_cov'] = smoothed.states.cov[0, :, 0,0]\n",
    "            df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "            df.loc[cond, 'lngDeg_cov'] = smoothed.states.cov[0, :, 1,1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e844106",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-07-11T12:13:06.793236Z",
     "iopub.status.busy": "2021-07-11T12:13:06.792587Z",
     "iopub.status.idle": "2021-07-11T12:14:57.700841Z",
     "shell.execute_reply": "2021-07-11T12:14:57.701422Z",
     "shell.execute_reply.started": "2021-07-11T11:43:37.487682Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 111.13231,
     "end_time": "2021-07-11T12:14:57.701611",
     "exception": false,
     "start_time": "2021-07-11T12:13:06.569301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ro = add_distance_diff(test).copy()\n",
    "th = 50\n",
    "print('outlier filtering')\n",
    "test_ro.loc[((test_ro['dist_prev'] > th) & (test_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg','latv_pred','lngv_pred']\n",
    "print('appling kf smoothing')\n",
    "test_ro_kf = apply_kf_smoothing(test_ro[cols])\n",
    "print(get_train_score(test,gt))\n",
    "print(get_train_score(test_ro_kf,gt))\n",
    "velocity_kal = test_ro_kf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec1766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-11T12:14:58.155178Z",
     "iopub.status.busy": "2021-07-11T12:14:58.154172Z",
     "iopub.status.idle": "2021-07-11T12:14:58.862249Z",
     "shell.execute_reply": "2021-07-11T12:14:58.861443Z",
     "shell.execute_reply.started": "2021-07-11T12:00:16.130095Z"
    },
    "papermill": {
     "duration": 0.93824,
     "end_time": "2021-07-11T12:14:58.862486",
     "exception": false,
     "start_time": "2021-07-11T12:14:57.924246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "en = velocity_kal.copy()\n",
    "cond = (velocity_kal['latv_pred'].apply(lambda x:np.abs(x))+velocity_kal['latv_pred'].apply(lambda x:np.abs(x)))>2e-4\n",
    "print(cond.sum()/len(en))\n",
    "en.loc[cond,'latDeg'] = vanilla_kal.loc[cond,'latDeg']*1.0\n",
    "en.loc[cond,'lngDeg'] = vanilla_kal.loc[cond,'lngDeg']*1.0\n",
    "print('通常カルマン:',get_train_score(vanilla_kal,gt))\n",
    "print('速度カルマン',get_train_score(velocity_kal,gt))\n",
    "print('早いところ通常カルマン、遅いところは速度カルマン:',get_train_score(en,gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fc6c2",
   "metadata": {
    "papermill": {
     "duration": 0.221501,
     "end_time": "2021-07-11T12:14:59.307093",
     "exception": false,
     "start_time": "2021-07-11T12:14:59.085592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 考察\n",
    "- 0.05弱通常のカルマンフィルタより向上した（collectionでvalidationもしているのである程度効果はありそう）\n",
    " - Vanilla Kalman Filter:4.721716804543573\n",
    " - Velocity Kalman Filter:4.678559609622235\n",
    "- アンサンブルに関しては効果はほぼない\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c09d19",
   "metadata": {
    "papermill": {
     "duration": 0.222522,
     "end_time": "2021-07-11T12:14:59.752001",
     "exception": false,
     "start_time": "2021-07-11T12:14:59.529479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 447.942355,
   "end_time": "2021-07-11T12:15:01.940977",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-11T12:07:33.998622",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}