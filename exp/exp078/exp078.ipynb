{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from scipy.spatial.distance import cdist\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from cv2 import Rodrigues\n",
    "from math import sin, cos, atan2, sqrt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Start Logging...\n"
     ]
    }
   ],
   "source": [
    "def init_logger(log_file='logger.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "    \n",
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "logger = init_logger(log_file=f'./{today}.log')\n",
    "logger.info('Start Logging...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pickle\n",
    "def to_pickle(filename, obj):\n",
    "    with open(filename, mode='wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "def from_pickle(filename):\n",
    "    with open(filename, mode='rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    seed = 1996\n",
    "    n_splits = 5\n",
    "    use_folds = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# globals variable\n",
    "config = Config()\n",
    "SEED = config.seed\n",
    "N_SPLITS = config.n_splits\n",
    "USE_FOLDS = config.use_folds\n",
    "EXP_NAME = str(Path().resolve()).split('/')[-1]\n",
    "\n",
    "# waypointを補正したdataset\n",
    "root_dir = Path('../../input')\n",
    "data_dir = root_dir/'google-smartphone-decimeter-challenge'\n",
    "collection_type_dict = from_pickle(root_dir/\"road_type.pkl\") # by edge'sdegree and highway feature\n",
    "collection_type_dict[\"test\"]['2021-03-25-US-PAO-1'] = \"normal\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'train': {'2020-05-14-US-MTV-1': 'highway',\n",
       "  '2020-05-14-US-MTV-2': 'highway',\n",
       "  '2020-05-21-US-MTV-1': 'highway',\n",
       "  '2020-05-21-US-MTV-2': 'highway',\n",
       "  '2020-05-29-US-MTV-1': 'highway',\n",
       "  '2020-05-29-US-MTV-2': 'highway',\n",
       "  '2020-06-04-US-MTV-1': 'highway',\n",
       "  '2020-06-05-US-MTV-1': 'highway',\n",
       "  '2020-06-05-US-MTV-2': 'highway',\n",
       "  '2020-06-11-US-MTV-1': 'highway',\n",
       "  '2020-07-08-US-MTV-1': 'highway',\n",
       "  '2020-07-17-US-MTV-1': 'highway',\n",
       "  '2020-07-17-US-MTV-2': 'highway',\n",
       "  '2020-08-03-US-MTV-1': 'highway',\n",
       "  '2020-08-06-US-MTV-2': 'highway',\n",
       "  '2020-09-04-US-SF-1': 'highway',\n",
       "  '2020-09-04-US-SF-2': 'highway',\n",
       "  '2021-01-04-US-RWC-1': 'highway',\n",
       "  '2021-01-04-US-RWC-2': 'highway',\n",
       "  '2021-01-05-US-SVL-1': 'highway',\n",
       "  '2021-01-05-US-SVL-2': 'highway',\n",
       "  '2021-03-10-US-SVL-1': 'normal',\n",
       "  '2021-04-15-US-MTV-1': 'normal',\n",
       "  '2021-04-22-US-SJC-1': 'city',\n",
       "  '2021-04-26-US-SVL-1': 'normal',\n",
       "  '2021-04-28-US-MTV-1': 'normal',\n",
       "  '2021-04-28-US-SJC-1': 'city',\n",
       "  '2021-04-29-US-MTV-1': 'normal',\n",
       "  '2021-04-29-US-SJC-2': 'city'},\n",
       " 'test': {'2020-05-15-US-MTV-1': 'highway',\n",
       "  '2020-05-28-US-MTV-1': 'highway',\n",
       "  '2020-05-28-US-MTV-2': 'highway',\n",
       "  '2020-06-04-US-MTV-2': 'highway',\n",
       "  '2020-06-10-US-MTV-1': 'highway',\n",
       "  '2020-06-10-US-MTV-2': 'highway',\n",
       "  '2020-08-03-US-MTV-2': 'highway',\n",
       "  '2020-08-13-US-MTV-1': 'highway',\n",
       "  '2021-03-16-US-MTV-2': 'highway',\n",
       "  '2021-03-16-US-RWC-2': 'normal',\n",
       "  '2021-03-25-US-PAO-1': 'normal',\n",
       "  '2021-04-02-US-SJC-1': 'highway',\n",
       "  '2021-04-08-US-MTV-1': 'normal',\n",
       "  '2021-04-21-US-MTV-1': 'normal',\n",
       "  '2021-04-22-US-SJC-2': 'city',\n",
       "  '2021-04-26-US-SVL-2': 'normal',\n",
       "  '2021-04-28-US-MTV-2': 'normal',\n",
       "  '2021-04-29-US-MTV-2': 'normal',\n",
       "  '2021-04-29-US-SJC-3': 'city'}}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "collection_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_ground_truth(args):\n",
    "    (collectionName, phoneName), df = args\n",
    "    \n",
    "    path = data_dir / f\"train/{collectionName}/{phoneName}/ground_truth.csv\"\n",
    "    target_df = pd.read_csv(path)\n",
    "    output_df = pd.DataFrame()\n",
    "    # merge derived and target by 'millisSinceGpsEpoch'\n",
    "    for epoch, epoch_df in df.groupby('millisSinceGpsEpoch'):\n",
    "        idx = (target_df['millisSinceGpsEpoch'] - epoch).abs().argmin()\n",
    "        epoch_diff = epoch - target_df.loc[idx, 'millisSinceGpsEpoch']\n",
    "        epoch_df['epoch_diff'] = epoch_diff\n",
    "        epoch_df['target_latDeg'] = target_df.loc[idx, 'latDeg']\n",
    "        epoch_df['target_lngDeg'] = target_df.loc[idx, 'lngDeg']\n",
    "        epoch_df['speedMps'] = target_df.loc[idx, 'speedMps']\n",
    "        output_df = pd.concat([output_df, epoch_df]).reset_index(drop=True)    \n",
    "    return output_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def visualize_trafic(df):\n",
    "    fig = px.scatter_mapbox(df,\n",
    "                            \n",
    "                            # Here, plotly gets, (x,y) coordinates\n",
    "                            lat=\"latDeg\",\n",
    "                            lon=\"lngDeg\",\n",
    "                            text='phoneName',\n",
    "                            \n",
    "                            #Here, plotly detects color of series\n",
    "                            color=\"collectionName\",\n",
    "                            labels=\"collectionName\",\n",
    "                            \n",
    "                            zoom=9,\n",
    "                            center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                            height=600,\n",
    "                            width=800)\n",
    "    fig.update_layout(mapbox_style='stamen-terrain')\n",
    "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "    fig.update_layout(title_text=\"GPS trafic\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def visualize_collection(df, hover_data=[\"elapsed_epoch\", \"speedMps\"], collection_name='2021-04-29-US-SJC-2'):\n",
    "    gt_df = pd.read_csv(root_dir / 'all_ground_truth.csv')\n",
    "    gt_df['phoneName'] = 'ground truth'\n",
    "    gt_df = pd.concat([df, gt_df])\n",
    "    target_df = gt_df[gt_df['collectionName']==collection_name]   \n",
    "    target_df['elapsed_epoch'] = ((target_df['millisSinceGpsEpoch'] - target_df['millisSinceGpsEpoch'].min()) / 1000)\n",
    "    fig = px.scatter_mapbox(target_df,\n",
    "                                \n",
    "                            # Here, plotly gets, (x,y) coordinates\n",
    "                            lat=\"latDeg\",\n",
    "                            lon=\"lngDeg\",\n",
    "                                \n",
    "                            #Here, plotly detects color of series\n",
    "                            color=\"phoneName\",\n",
    "                            labels=\"phoneName\",\n",
    "                            hover_data=hover_data,                \n",
    "\n",
    "                                \n",
    "                            zoom=9,\n",
    "                            center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                            height=600,\n",
    "                            width=800)\n",
    "    fig.update_layout(mapbox_style='stamen-terrain')\n",
    "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "    fig.update_layout(title_text=\"{collection_name}\")\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# metric\n",
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist\n",
    "\n",
    "def check_score(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"phone\" not in input_df.columns:\n",
    "        input_df[\"phone\"] = input_df[\"collectionName\"] + \"_\" + input_df[\"phoneName\"]\n",
    "\n",
    "    if \"target_latDeg\" not in input_df.columns:\n",
    "        processes = multiprocessing.cpu_count()\n",
    "        with multiprocessing.Pool(processes=processes) as pool:\n",
    "            gr = input_df.groupby(['collectionName','phoneName'])\n",
    "            dfs = pool.imap_unordered(get_ground_truth, gr)\n",
    "            dfs = tqdm(dfs, total=len(gr))\n",
    "            dfs = list(dfs)\n",
    "        input_df = pd.concat(dfs).sort_values(['collectionName', 'phoneName', 'millisSinceGpsEpoch']).reset_index(drop=True)     \n",
    "\n",
    "\n",
    "    output_df = input_df.copy()\n",
    "    \n",
    "    output_df['error'] = input_df.apply(\n",
    "        lambda r: calc_haversine(\n",
    "            r.latDeg, r.lngDeg, r.target_latDeg, r.target_lngDeg\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    meter_score = output_df['error'].mean()\n",
    "    logger.info(f'mean error: {meter_score}')\n",
    "\n",
    "    scores = []\n",
    "    p_50_scores = []\n",
    "    p_95_scores = []\n",
    "    mean_scores = []\n",
    "    phones = []\n",
    "    score_df = pd.DataFrame()\n",
    "    for phone in output_df['phone'].unique():\n",
    "        _index = output_df['phone']==phone\n",
    "        p_50 = np.percentile(output_df.loc[_index, 'error'], 50)\n",
    "        p_95 = np.percentile(output_df.loc[_index, 'error'], 95)\n",
    "        # print(f\"{phone} | 50:{p_50:.5g}| 95:{p_95:.5g}\")\n",
    "        p_50_scores.append(p_50)\n",
    "        p_95_scores.append(p_95)\n",
    "        mean_scores.append(np.mean([p_50, p_95]))\n",
    "        phones.append(phone)\n",
    "\n",
    "        scores.append(p_50)\n",
    "        scores.append(p_95)\n",
    "\n",
    "    score_df[\"phone\"] = phones\n",
    "    score_df[\"p_50_score\"] = p_50_scores\n",
    "    score_df[\"p_95_score\"] = p_95_scores\n",
    "    score_df[\"mean_score\"] = mean_scores\n",
    "    \n",
    "    comp_score = sum(scores) / len(scores)\n",
    "    logger.info(f\"competition metric:{comp_score}\")\n",
    "    return output_df, score_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_time(_df):\n",
    "    data = _df.copy().reset_index(drop=True)\n",
    "    data['ms_dif'] = data['millisSinceGpsEpoch']-data['millisSinceGpsEpoch'].shift()\n",
    "    mis_time = (len(data)-1-data['ms_dif'].value_counts()[1000.0])/len(data)\n",
    "    return mis_time>0.0\n",
    "\n",
    "def adjust_time(df, kf):\n",
    "    data = df.copy().reset_index(drop=True)\n",
    "    new_data = data.copy()\n",
    "    t = 'millisSinceGpsEpoch'\n",
    "    start = data['millisSinceGpsEpoch'].values[0]\n",
    "    end = data['millisSinceGpsEpoch'].values[-1]\n",
    "    ind = math.ceil((end-start)/1000)\n",
    "    _data = pd.DataFrame(index=range(ind))\n",
    "    _data['latDeg']=0\n",
    "    _data['lngDeg']=0\n",
    "    #_data['millisSinceGpsEpoch'] = 0\n",
    "    _data['millisSinceGpsEpoch'] = start+1000*_data.index\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in _data.index:\n",
    "        while _data.loc[i,t]<new_data.loc[cnt,t] or _data.loc[i,t]>new_data.loc[cnt+1,t]:\n",
    "            cnt+=1\n",
    "        if _data.loc[i,t]==new_data.loc[cnt,t]:\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']\n",
    "        else:\n",
    "            w=(new_data.loc[cnt+1,t]-_data.loc[i,t])/(new_data.loc[cnt+1,t]-new_data.loc[cnt,t])\n",
    "            if w<0 or w>1:\n",
    "                print(w)\n",
    "            _data.loc[i,'latDeg'] = new_data.loc[cnt,'latDeg']*w+new_data.loc[cnt+1,'latDeg']*(1-w)\n",
    "            _data.loc[i,'lngDeg'] = new_data.loc[cnt,'lngDeg']*w+new_data.loc[cnt+1,'lngDeg']*(1-w)\n",
    "    _data.head()\n",
    "    __data = _data.loc[:,['latDeg', 'lngDeg']].to_numpy()\n",
    "    __data = __data.reshape(1, len(__data), 2)\n",
    "    smoothed = kf.smooth(__data)\n",
    "    _data.loc[:,'latDeg_pred'] = smoothed.states.mean[0, :, 0]\n",
    "    _data.loc[:,'lngDeg_pred'] = smoothed.states.mean[0, :, 1]\n",
    "    _data.loc[:,'latDeg_cov_pred'] = smoothed.states.cov[0, :, 0,0]\n",
    "    _data.loc[:,'lngDeg_cov_pred'] = smoothed.states.cov[0, :, 1,1]\n",
    "    data['latDeg_cov'] = 1\n",
    "    data['lngDeg_cov'] = 1\n",
    "\n",
    "    cnt = 0\n",
    "    for i in data.index:\n",
    "        try:\n",
    "            while data.loc[i,t]<_data.loc[cnt,t] or data.loc[i,t]>_data.loc[cnt+1,t]:\n",
    "                cnt+=1\n",
    "            if data.loc[i,t]==_data.loc[cnt,t]:\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']\n",
    "            else:\n",
    "                w=(_data.loc[cnt+1,t]-data.loc[i,t])/(_data.loc[cnt+1,t]-_data.loc[cnt,t])\n",
    "                if w<0 or w>1:\n",
    "                    print(w)\n",
    "                data.loc[i,'latDeg'] = _data.loc[cnt,'latDeg_pred']*w+_data.loc[cnt+1,'latDeg_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg'] = _data.loc[cnt,'lngDeg_pred']*w+_data.loc[cnt+1,'lngDeg_pred']*(1-w)\n",
    "                data.loc[i,'latDeg_cov'] = _data.loc[cnt,'latDeg_cov_pred']*w+_data.loc[cnt+1,'latDeg_cov_pred']*(1-w)\n",
    "                data.loc[i,'lngDeg_cov'] = _data.loc[cnt,'lngDeg_cov_pred']*w+_data.loc[cnt+1,'lngDeg_cov_pred']*(1-w)\n",
    "        except:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import simdkalman\n",
    "\n",
    "def apply_kf_smoothing(input_df, phase=\"train\", target_type=[\"highway\", \"normal\", \"city\"]):\n",
    "    logger.info('[START] Kalman Smoothing')\n",
    "    # define kf model\n",
    "    df_list = []\n",
    "    for (collection_name,phone_name), df in input_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        if collection_type_dict[phase][collection_name] == \"highway\":\n",
    "            # score: 2.8204355271575277 -> 2.8193315973589166\n",
    "            # Best hyperparameters: {'T': 2.0, 'p_noise': 2.705460166533897e-07, 'o_noise': 1.3966292122575907e-09}\n",
    "            # defaultを使用\n",
    "            T = 1.0\n",
    "            p_noise = 1e-9\n",
    "            o_noise = 1e-9\n",
    "\n",
    "            state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                                        [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "            process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * p_noise\n",
    "            observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])    \n",
    "            observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * o_noise\n",
    "\n",
    "        elif collection_type_dict[phase][collection_name] == \"normal\":\n",
    "            # score: 5.278659054251117 -> 5.213777593582035\n",
    "            # Best hyperparameters: {'T': 0.9, 'p_noise': 1.0380420148601327e-12, 'o_noise': 6.122291790973238e-06}\n",
    "            T = 0.9\n",
    "            p_noise = 1e-12\n",
    "            o_noise = 6.1e-6\n",
    "\n",
    "            state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                                        [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "            process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * p_noise\n",
    "            observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])    \n",
    "            observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * o_noise\n",
    "        else:\n",
    "            # score: 18.002721439031802 -> 17.71027356807068\n",
    "            # Best hyperparameters: {'T': 0.30000000000000004, 'p_noise': 1.4040850185364873e-12, 'o_noise': 1.2221845582747804e-08}  \n",
    "            T = 0.3\n",
    "            p_noise = 1.4e-12\n",
    "            o_noise = 1.2e-8\n",
    "\n",
    "            state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                                        [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "            process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * p_noise\n",
    "            observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])    \n",
    "            observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * o_noise\n",
    "\n",
    "        if collection_type_dict[phase][collection_name] in target_type:\n",
    "            kf = simdkalman.KalmanFilter(\n",
    "                state_transition=state_transition,\n",
    "                process_noise=process_noise,\n",
    "                observation_model=observation_model,\n",
    "                observation_noise=observation_noise)\n",
    "\n",
    "\n",
    "            if check_time(df):\n",
    "                _df = adjust_time(df, kf)\n",
    "                df['latDeg'] = _df['latDeg'].values\n",
    "                df['lngDeg'] = _df['lngDeg'].values\n",
    "                df['latDeg_cov'] = _df['latDeg_cov'].values\n",
    "                df['lngDeg_cov'] = _df['lngDeg_cov'].values\n",
    "            else:\n",
    "                data = df[['latDeg', 'lngDeg']].to_numpy()\n",
    "                data = data.reshape(1, len(data), 2)\n",
    "                smoothed = kf.smooth(data)\n",
    "                df['latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "                df['latDeg_cov'] = smoothed.states.cov[0, :, 0,0]\n",
    "                df['lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "                df['lngDeg_cov'] = smoothed.states.cov[0, :, 1,1]\n",
    "            df_list.append(df)\n",
    "        else:\n",
    "            df_list.append(df)\n",
    "    output_df = pd.concat(df_list).reset_index(drop=True) # .sort_index()\n",
    "    return output_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# https://www.kaggle.com/dehokanta/baseline-post-processing-by-outlier-correction\n",
    "def outlier_correction(df):\n",
    "    logger.info('[START] outlier correction')\n",
    "    df['dist_pre'] = 0\n",
    "    df['dist_pro'] = 0\n",
    "    df['latDeg_pre'] = df['latDeg'].shift(periods=1,fill_value=0)\n",
    "    df['lngDeg_pre'] = df['lngDeg'].shift(periods=1,fill_value=0)\n",
    "    df['latDeg_pro'] = df['latDeg'].shift(periods=-1,fill_value=0)\n",
    "    df['lngDeg_pro'] = df['lngDeg'].shift(periods=-1,fill_value=0)\n",
    "    df['dist_pre'] = calc_haversine(df[\"latDeg_pre\"], df[\"lngDeg_pre\"], df[\"latDeg\"], df[\"lngDeg\"])\n",
    "    df['dist_pro'] = calc_haversine(df[\"latDeg\"], df[\"lngDeg\"], df[\"latDeg_pro\"], df[\"lngDeg_pro\"])\n",
    "\n",
    "    list_phone = df['phone'].unique()\n",
    "    for phone in list_phone:\n",
    "        ind_s = df[df['phone'] == phone].index[0]\n",
    "        ind_e = df[df['phone'] == phone].index[-1]\n",
    "        df.loc[ind_s,'dist_pre'] = 0\n",
    "        df.loc[ind_e,'dist_pro'] = 0\n",
    "\n",
    "    pro_95 = df['dist_pro'].mean() + (df['dist_pro'].std() * 2)\n",
    "    pre_95 = df['dist_pre'].mean() + (df['dist_pre'].std() * 2)\n",
    "    ind = df[(df['dist_pro'] > pro_95)&(df['dist_pre'] > pre_95)][['dist_pre','dist_pro']].index\n",
    "\n",
    "    for i in ind:\n",
    "        df.loc[i,'latDeg'] = (df.loc[i-1,'latDeg'] + df.loc[i+1,'latDeg'])/2\n",
    "        df.loc[i,'lngDeg'] = (df.loc[i-1,'lngDeg'] + df.loc[i+1,'lngDeg'])/2\n",
    "    return df\n",
    "\n",
    "\n",
    "def linear_interpolation(input_df, speed_thr=45):\n",
    "    logger.info('[START] linear interpolation')\n",
    "    dfs = pd.DataFrame()\n",
    "    use_col = input_df.columns\n",
    "    for (collectionName, phoneName), df in input_df.groupby(['collectionName', 'phoneName']):\n",
    "\n",
    "        df['delta'] = calc_haversine(\n",
    "            df['latDeg'], df['lngDeg'], df['latDeg'].shift(1), df['lngDeg'].shift(1))\n",
    "        df['time_delta'] = df['millisSinceGpsEpoch'] - \\\n",
    "            df['millisSinceGpsEpoch'].shift(1)\n",
    "        df['delta'].fillna(0, inplace=True)\n",
    "        df['time_delta'].fillna(0, inplace=True)\n",
    "        df['speed'] = df['delta'] / (df['time_delta']/1000)  # m/s\n",
    "        df['speed'].fillna(0, inplace=True)\n",
    "\n",
    "        # 一度欠損値にする\n",
    "        df.loc[speed_thr < df['speed'], ['latDeg', 'lngDeg']] = np.nan\n",
    "        df['dummy_datetime'] = pd.to_datetime(df['millisSinceGpsEpoch'])\n",
    "        df = df.set_index('dummy_datetime')\n",
    "\n",
    "        # 時間に合わせて線形補間\n",
    "        df = df.interpolate(method='time').reset_index(drop=True)\n",
    "        dfs = pd.concat([dfs, df]).reset_index(drop=True)\n",
    "    return dfs[use_col]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def make_lerp_data(df):\n",
    "    '''\n",
    "    Generate interpolated lat,lng values for different phone times in the same collection.\n",
    "    '''\n",
    "    org_columns = df.columns\n",
    "\n",
    "    # Generate a combination of time x collection x phone and combine it with the original data (generate records to be interpolated)\n",
    "    time_list = df[['collectionName', 'millisSinceGpsEpoch']].drop_duplicates()\n",
    "    phone_list = df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "    tmp = time_list.merge(phone_list, on='collectionName', how='outer')\n",
    "\n",
    "    lerp_df = tmp.merge(\n",
    "        df, on=['collectionName', 'millisSinceGpsEpoch', 'phoneName'], how='left')\n",
    "\n",
    "    lerp_df['phone'] = lerp_df['collectionName'] + '_' + lerp_df['phoneName']\n",
    "    lerp_df = lerp_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n",
    "\n",
    "    # linear interpolation\n",
    "    lerp_df['latDeg_prev'] = lerp_df['latDeg'].shift(1)\n",
    "    lerp_df['latDeg_next'] = lerp_df['latDeg'].shift(-1)\n",
    "    lerp_df['lngDeg_prev'] = lerp_df['lngDeg'].shift(1)\n",
    "    lerp_df['lngDeg_next'] = lerp_df['lngDeg'].shift(-1)\n",
    "    lerp_df['phone_prev'] = lerp_df['phone'].shift(1)\n",
    "    lerp_df['phone_next'] = lerp_df['phone'].shift(-1)\n",
    "    lerp_df['time_prev'] = lerp_df['millisSinceGpsEpoch'].shift(1)\n",
    "    lerp_df['time_next'] = lerp_df['millisSinceGpsEpoch'].shift(-1)\n",
    "\n",
    "    # Leave only records to be interpolated(missing coords data)\n",
    "    lerp_df = lerp_df[(lerp_df['latDeg'].isnull()) & (lerp_df['phone'] == lerp_df['phone_prev']) & (\n",
    "        lerp_df['phone'] == lerp_df['phone_next'])].copy()\n",
    "    # calc lerp\n",
    "    lerp_df['latDeg'] = lerp_df['latDeg_prev'] + ((lerp_df['latDeg_next'] - lerp_df['latDeg_prev']) * (\n",
    "        (lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev'])))\n",
    "    lerp_df['lngDeg'] = lerp_df['lngDeg_prev'] + ((lerp_df['lngDeg_next'] - lerp_df['lngDeg_prev']) * (\n",
    "        (lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev'])))\n",
    "\n",
    "    # Leave only the data that has a complete set of previous and next data.\n",
    "    lerp_df = lerp_df[~lerp_df['latDeg'].isnull()]\n",
    "\n",
    "    return lerp_df[org_columns]\n",
    "\n",
    "def calc_mean_pred(df, lerp_df):\n",
    "    '''\n",
    "    Make a prediction based on the average of the predictions of phones in the same collection.\n",
    "    '''\n",
    "    add_lerp = pd.concat([df, lerp_df])\n",
    "    mean_pred_result = add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])[\n",
    "        ['latDeg', 'lngDeg']].mean().reset_index()\n",
    "    mean_pred_df = df.copy()\n",
    "    mean_pred_df = mean_pred_df.drop([\"latDeg\", \"lngDeg\"], axis=1)\n",
    "    mean_pred_df = mean_pred_df.merge(mean_pred_result[['collectionName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=[\n",
    "                                      'collectionName', 'millisSinceGpsEpoch'], how='left')\n",
    "    return mean_pred_df\n",
    "\n",
    "def apply_mean(df):\n",
    "    logger.info('[START] phone-mean')\n",
    "    lerp = make_lerp_data(df)\n",
    "    mean_df = calc_mean_pred(df, lerp)\n",
    "    return mean_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_removedevice(input_df: pd.DataFrame, device: str) -> pd.DataFrame:\n",
    "    logger.info('[START] remove device')\n",
    "    input_df['index'] = input_df.index\n",
    "    input_df = input_df.sort_values('millisSinceGpsEpoch')\n",
    "    input_df.index = input_df['millisSinceGpsEpoch'].values\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    for _, subdf in input_df.groupby('collectionName'):\n",
    "\n",
    "        phones = subdf['phoneName'].unique()\n",
    "\n",
    "        if (len(phones) == 1) or (not device in phones):\n",
    "            output_df = pd.concat([output_df, subdf])\n",
    "            continue\n",
    "\n",
    "        origin_df = subdf.copy()\n",
    "\n",
    "        _index = subdf['phoneName'] == device\n",
    "        subdf.loc[_index, 'latDeg'] = np.nan\n",
    "        subdf.loc[_index, 'lngDeg'] = np.nan\n",
    "        subdf = subdf.interpolate(method='index', limit_area='inside')\n",
    "\n",
    "        _index = subdf['latDeg'].isnull()\n",
    "        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n",
    "        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n",
    "\n",
    "        output_df = pd.concat([output_df, subdf])\n",
    "\n",
    "    output_df.index = output_df['index'].values\n",
    "    output_df = output_df.sort_index()\n",
    "\n",
    "    del output_df['index']\n",
    "\n",
    "    return output_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_mean(input_df, max_speed_thr = 0.95, min_second_thr=0, max_second_thr = 2, phase=\"train\"):\n",
    "    \"\"\"\n",
    "    # 予測した速度がmax_speed_thr以下のものがmax_second_thr(s)以上連続する区間を停止区間とみなして平均をとる。\n",
    "    TODO 道路タイプで適用可否決めたほうがいいかも。都市のところは開始、終了だけにするとか。\n",
    "    \"\"\"\n",
    "    logger.info('[START] stop mean')\n",
    "    use_col = input_df.columns\n",
    "    output_df = pd.DataFrame()\n",
    "    for collection_name, df in input_df.groupby(['collectionName']):\n",
    "\n",
    "        df = df.sort_values('millisSinceGpsEpoch')\n",
    "        df['flag'] = df['pred_speedMps'] < max_speed_thr\n",
    "        df['stop_group'] = (df['flag'] != df['flag'].shift()).cumsum()\n",
    "        df['num_flaged_data'] = df.groupby('stop_group')['flag'].transform(sum)  # stop group内でのflag(stopしていると判定された)の数\n",
    "        df.loc[(min_second_thr <= df['num_flaged_data'])&(df['num_flaged_data'] < max_second_thr), 'stop_group'] = 0\n",
    "\n",
    "        for i in df['stop_group'].unique():\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            if collection_type_dict[phase][collection_name]=='city':\n",
    "                # when start or end\n",
    "                if (i == 1)or(i == df['stop_group'].unique()[-1]):\n",
    "                    df.loc[df['stop_group']==i, 'latDeg'] = df.loc[df['stop_group']==i, 'latDeg'].mean()\n",
    "                    df.loc[df['stop_group']==i, 'lngDeg'] = df.loc[df['stop_group']==i, 'lngDeg'].mean()\n",
    "            else:\n",
    "                df.loc[df['stop_group']==i, 'latDeg'] = df.loc[df['stop_group']==i, 'latDeg'].mean()\n",
    "                df.loc[df['stop_group']==i, 'lngDeg'] = df.loc[df['stop_group']==i, 'lngDeg'].mean()\n",
    "        \n",
    "        output_df = pd.concat([output_df, df])\n",
    "    return output_df[use_col].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# train_points = pd.read_csv('../../input/all_ground_truth.csv')\n",
    "# test_points = pd.read_csv('../../input/road_points.csv')\n",
    "# # road_points[\"place\"] = road_points[\"collectionName\"].str[11:]\n",
    "# road_points = pd.concat([train_points, test_points])\n",
    "# road_points = road_points[[\"lngDeg\", \"latDeg\"]]\n",
    "# road_points\n",
    "\n",
    "\n",
    "road_points = pd.read_csv('../../input/all_ground_truth.csv')\n",
    "print(road_points.shape)\n",
    "dfs = []\n",
    "for (collectionName, phoneName), df in road_points.groupby([\"collectionName\", \"phoneName\"]):\n",
    "    if collection_type_dict[\"train\"][collectionName] == \"city\":\n",
    "        # 前後の時間の平均の時の座標を補完する\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        tmp_df = pd.DataFrame(((df[\"millisSinceGpsEpoch\"] + df[\"millisSinceGpsEpoch\"].shift(1)) / 2).dropna())\n",
    "        tmp_df[\"collectionName\"] = collectionName\n",
    "        tmp_df[\"phoneName\"] = phoneName\n",
    "        df = pd.concat([df, tmp_df]).sort_values(\"millisSinceGpsEpoch\")\n",
    "        df = df.set_index(pd.to_datetime(df[\"millisSinceGpsEpoch\"])).interpolate().reset_index(drop=True)\n",
    "\n",
    "        # 前後の時間の平均の時の座標を補完する\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        tmp_df = pd.DataFrame(((df[\"millisSinceGpsEpoch\"] + df[\"millisSinceGpsEpoch\"].shift(1)) / 2).dropna())\n",
    "        tmp_df[\"collectionName\"] = collectionName\n",
    "        tmp_df[\"phoneName\"] = phoneName\n",
    "        df = pd.concat([df, tmp_df]).sort_values(\"millisSinceGpsEpoch\")\n",
    "        df = df.set_index(pd.to_datetime(df[\"millisSinceGpsEpoch\"])).interpolate().reset_index(drop=True)\n",
    "\n",
    "\n",
    "        df = df[[\"lngDeg\", \"latDeg\"]]\n",
    "        dfs.append(df)\n",
    "\n",
    "road_points = pd.concat(dfs).reset_index(drop=True).drop_duplicates(subset=[\"latDeg\", \"lngDeg\"])\n",
    "print(road_points.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(51260, 11)\n(24908, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_mapbox(road_points,                              \n",
    "#                         # Here, plotly gets, (x,y) coordinates\n",
    "#                         lat=\"latDeg\",\n",
    "#                         lon=\"lngDeg\",\n",
    "                            \n",
    "#                         zoom=9,\n",
    "#                         center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "#                         height=600,\n",
    "#                         width=800)\n",
    "# fig.update_layout(mapbox_style='stamen-terrain')\n",
    "# fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# # fig.update_layout(title_text=\"{collection_name}\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_mean(input_df):\n",
    "    logger.info('[START] epoch mean')\n",
    "    output_df = input_df.copy()\n",
    "    output_df['sSinceGpsEpoch'] = np.round(output_df['millisSinceGpsEpoch'] / 1000, 1)\n",
    "    print(output_df['millisSinceGpsEpoch'].nunique(), output_df['sSinceGpsEpoch'].nunique())\n",
    "    output_df = pd.merge(output_df, output_df.groupby('sSinceGpsEpoch')['latDeg', 'lngDeg'].mean().reset_index().rename(columns={'latDeg':'_latDeg', 'lngDeg':'_lngDeg'}), on='sSinceGpsEpoch', how='left')\n",
    "    output_df = output_df.drop(['latDeg','lngDeg'], axis=1).rename(columns={'_latDeg':'latDeg', '_lngDeg':'lngDeg'})\n",
    "    # assert df.columns == use_col\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_road_data(input_df, phase):\n",
    "    # make road dataframe\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    gr = input_df.groupby(\"collectionName\")\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(extract_road_data, gr)\n",
    "        dfs = tqdm(dfs, total=len(gr))\n",
    "        dfs = list(dfs)\n",
    "    road_df = pd.concat(dfs)\n",
    "\n",
    "    road_type_dict = {\n",
    "    'unclassified': \"1_residential\",\n",
    "    'residential': \"1_residential\",\n",
    "    'tertiary_link': \"2_tertaiary\", \n",
    "    'tertiary': \"2_tertaiary\", \n",
    "    'secondary_link': '3_secondary', \n",
    "    'secondary': '3_secondary', \n",
    "    'primary_link': '4_primary', \n",
    "    'primary': '4_primary', \n",
    "    'trunk_link': '5_trunk',\n",
    "    'trunk': '5_trunk',\n",
    "    'motorway_link': '6_motorway',\n",
    "    'motorway': '6_motorway',\n",
    "    }\n",
    "\n",
    "    road_df[\"highway\"] = [i[0] if type(i)==list else i for i in road_df[\"highway\"]]\n",
    "    road_df[\"road_type\"] = road_df[\"highway\"].replace(road_type_dict)\n",
    "    road_df[\"geometry\"] = road_df[\"geometry\"].buffer(0.0001)  # linestring -> polygon\n",
    "    return road_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_road_type(x, target_road_df):\n",
    "    p = x[\"geometry\"]\n",
    "    for idx, row in target_road_df.iterrows():\n",
    "        if row[\"geometry\"].contains(p):\n",
    "            return row[\"road_type\"]\n",
    "    \n",
    "def apply_road_type(args):\n",
    "    (collection_name, df), road_df = args\n",
    "    target_road_df = road_df[road_df[\"collectionName\"]==collection_name]\n",
    "    df[\"road_type\"] = df.apply(judge_road_type, target_road_df=target_road_df, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_road_type(input_df, phase=\"train\"):\n",
    "    output_df = input_df.copy()\n",
    "    use_col = list(input_df.columns)\n",
    "\n",
    "    road_df = make_road_data(input_df, phase)\n",
    "    output_df[\"geometry\"] = [Point(lng, lat) for lng, lat in output_df[[\"lngDeg\", \"latDeg\"]].to_numpy()]\n",
    "    output_df = gpd.GeoDataFrame(output_df, geometry=\"geometry\")\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    n = output_df[\"collectionName\"].nunique()\n",
    "    gr = zip(output_df.groupby(\"collectionName\"), [road_df]*n)\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(apply_road_type, gr)\n",
    "        dfs = tqdm(dfs, total=n)\n",
    "        dfs = list(dfs)\n",
    "    output_df = pd.concat(dfs)\n",
    "    output_df[\"road_type\"].fillna(\"0_unknown\", inplace=True)\n",
    "    return output_df[use_col + [\"road_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import osmnx as ox\n",
    "import momepy\n",
    "import geopandas as gpd\n",
    "from shapely.ops import nearest_points\n",
    "import momepy\n",
    "\n",
    "def extract_road_data(args):\n",
    "    collection_name, points = args\n",
    "    ################\n",
    "    # read load data\n",
    "    ################\n",
    "    points[\"geometry\"] = [Point(p) for p in points[[\"lngDeg\", \"latDeg\"]].to_numpy()]\n",
    "    points = gpd.GeoDataFrame(points, geometry=points[\"geometry\"])\n",
    "    offset = 0.1**3\n",
    "    bbox = points.bounds + [-offset, -offset, offset, offset]\n",
    "    east = bbox[\"minx\"].min()\n",
    "    west = bbox[\"maxx\"].max()\n",
    "    south = bbox[\"miny\"].min()\n",
    "    north = bbox[\"maxy\"].max()\n",
    "    G = ox.graph.graph_from_bbox(north, south, east, west, network_type='drive')\n",
    "    _, lines = momepy.nx_to_gdf(G)\n",
    "    lines = lines.dropna(subset=[\"geometry\"]).reset_index(drop=True)\n",
    "    hits = bbox.apply(lambda row: list(lines.sindex.intersection(row)), axis=1)\n",
    "    tmp = pd.DataFrame({\n",
    "        # index of points table\n",
    "        \"pt_idx\": np.repeat(hits.index, hits.apply(len)),\n",
    "        # ordinal position of line - access via iloc later\n",
    "        \"line_i\": np.concatenate(hits.values)\n",
    "    })\n",
    "    # Join back to the lines on line_i; we use reset_index() to \n",
    "    # give us the ordinal position of each line\n",
    "    tmp = tmp.join(lines.reset_index(drop=True), on=\"line_i\")\n",
    "    # Join back to the original points to get their geometry\n",
    "    # rename the point geometry as \"point\"\n",
    "    tmp = tmp.join(points.geometry.rename(\"point\"), on=\"pt_idx\")\n",
    "    # Convert back to a GeoDataFrame, so we can do spatial ops\n",
    "    tmp = gpd.GeoDataFrame(tmp, geometry=\"geometry\", crs=points.crs)\n",
    "\n",
    "    ####################\n",
    "    # Find closest line\n",
    "    ###################\n",
    "    tmp[\"snap_dist\"] = tmp.geometry.distance(gpd.GeoSeries(tmp.point))\n",
    "\n",
    "    # Discard any lines that are greater than tolerance from points\n",
    "    tolerance = 0.0001 # 0.0005\n",
    "    tmp = tmp.loc[tmp.snap_dist <= tolerance]\n",
    "    # Sort on ascending snap distance, so that closest goes to top\n",
    "    tmp = tmp.sort_values(by=[\"snap_dist\"])\n",
    "\n",
    "    # group by the index of the points and take the first, which is the\n",
    "    # closest line \n",
    "    closest = tmp.groupby(\"pt_idx\").first()\n",
    "    # construct a GeoDataFrame of the closest lines\n",
    "    # closest = gpd.GeoDataFrame(closest, geometry=\"geometry\")\n",
    "    closest = closest.drop_duplicates(\"line_i\").reset_index(drop=True)\n",
    "\n",
    "    closest.drop(['point','snap_dist'], axis=1, inplace=True)\n",
    "    closest['collectionName'] = collection_name\n",
    "    return closest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_to_grid(df, min_thr, max_thr):\n",
    "    \"\"\"\n",
    "    Snap to grid if within a threshold.\n",
    "    \n",
    "    x, y are the predicted points.\n",
    "    x_, y_ are the closest grid points.\n",
    "    _x_, _y_ are the new predictions after post processing.\n",
    "    \"\"\"\n",
    "    df['_latDeg_'] = df['latDeg']\n",
    "    df['_lngDeg_'] = df['lngDeg']\n",
    "    df[\"dist\"] = calc_haversine(df[\"latDeg\"], df[\"lngDeg\"], df[\"latDeg_\"], df[\"lngDeg_\"])\n",
    "    query = (min_thr <= df['dist'])&(df['dist'] < max_thr)\n",
    "    df.loc[query, '_latDeg_'] = df.loc[query, 'latDeg_']\n",
    "    df.loc[query, '_lngDeg_'] = df.loc[query, 'lngDeg_']\n",
    "\n",
    "    df = df.drop([\"latDeg_\", \"lngDeg_\", \"latDeg\", \"lngDeg\"], axis=1).rename(columns={\"_latDeg_\":\"latDeg\", \"_lngDeg_\":\"lngDeg\"})\n",
    "    return df\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def add_xy(df):\n",
    "    df['point'] = [(x, y) for x,y in zip(df['lngDeg'], df['latDeg'])]\n",
    "    return df\n",
    "\n",
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    return points[cdist([point], points).argmin()]\n",
    "\n",
    "\n",
    "def matching_point(target_df, target_road_points=road_points):\n",
    "    # place = target_df[\"place\"].unique()[0]\n",
    "    # target_road_points = road_points[road_points[\"place\"]==place]\n",
    "    target_df['matched_point'] = [closest_point(x, list(target_road_points['point'])) for x in target_df['point']]\n",
    "    target_df['lngDeg_'] = target_df['matched_point'].apply(lambda x: x[0])\n",
    "    target_df['latDeg_'] = target_df['matched_point'].apply(lambda x: x[1])\n",
    "    return target_df\n",
    "\n",
    "\n",
    "def apply_snap_to_grid(df, road_points, min_thr, max_thr, phase=\"train\", target_type=[\"city\"]):\n",
    "    logger.info('[START] Snap to Grid')\n",
    "\n",
    "    df = add_xy(df)\n",
    "    road_points = add_xy(road_points)\n",
    "    df_list = []\n",
    "    ncpu = multiprocessing.cpu_count()\n",
    "    for collection_name, target_df in tqdm(df.groupby(\"collectionName\")):\n",
    "        if collection_type_dict[phase][collection_name] in target_type:\n",
    "            target_df = target_df.reset_index(drop=True)\n",
    "\n",
    "            k = round(len(target_df)/ncpu)\n",
    "            if k == 0:\n",
    "                k = 1\n",
    "            target_dfs = [target_df.loc[i:i+k-1, :] for i in range(0, len(target_df), k)]\n",
    "            with multiprocessing.Pool(processes=ncpu) as pool:\n",
    "                dfs = pool.imap_unordered(matching_point, target_dfs)\n",
    "                dfs = list(tqdm(dfs, total=len(target_dfs)))\n",
    "                target_df = pd.concat(dfs)\n",
    "            target_df = snap_to_grid(target_df, min_thr, max_thr)\n",
    "            df_list.append(target_df)\n",
    "        else:\n",
    "            df_list.append(target_df)\n",
    "\n",
    "    df = pd.concat(df_list).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_epoch(input_df):\n",
    "    df_list = []\n",
    "    for (collectionName, phoneName), df in input_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        df_list.append(df)\n",
    "    output_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_interpolate(input_df, thr=0.003, phase=\"train\", target_type=[\"city\"]):\n",
    "    logger.info(\"outlier to nan\")\n",
    "    df_list = []\n",
    "    for (collection_name, phone_name), df in input_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "        if collection_type_dict[phase][collection_name] in target_type:\n",
    "            df = df.sort_values(\"millisSinceGpsEpoch\").reset_index(drop=True)\n",
    "            df[\"dist\"] = calc_haversine(df[\"latDeg\"],df[\"lngDeg\"],df[\"latDeg\"].shift(1),df[\"lngDeg\"].shift(1)).values\n",
    "            df[\"delta_t\"] = df[\"millisSinceGpsEpoch\"] - df[\"millisSinceGpsEpoch\"].shift(1)\n",
    "            df[\"speed\"] = df[\"dist\"] / df[\"delta_t\"]\n",
    "            df[\"speed_pre_delta\"] = (df[\"speed\"] - df[\"speed\"].shift(1)).abs()\n",
    "            df[\"speed_post_delta\"] = (df[\"speed\"] - df[\"speed\"].shift(-1)).abs()\n",
    "            query = (df[\"speed_pre_delta\"]>=thr)&(df[\"speed_post_delta\"]>=thr)\n",
    "            print(collection_name, phone_name, len(df[query]))\n",
    "            df.loc[query, \"latDeg\"] = np.nan\n",
    "            df.loc[query, \"latDeg\"] = np.nan\n",
    "            # 時間に合わせて線形補間\n",
    "            df['dummy_datetime'] = pd.to_datetime(df['millisSinceGpsEpoch'])\n",
    "            df = df.set_index('dummy_datetime') \n",
    "            df = df.interpolate(method='time').reset_index(drop=True)\n",
    "        \n",
    "        df_list.append(df)\n",
    "    output_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_df = pd.read_csv(data_dir / \"baseline_locations_train_with_speed.csv\")\n",
    "# use_col = train_df.columns\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = linear_interpolation(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = outlier_correction(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = apply_kf_smoothing(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = apply_mean(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = get_removedevice(train_df, 'Pixel4')\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = get_removedevice(train_df, 'SamsungS20Ultra')\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = get_removedevice(train_df, 'Pixel4XL')\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = apply_snap_to_grid(train_df, road_points, min_thr=0, max_thr=100, phase=\"train\", target_type=[\"city\"])\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = outlier_interpolate(train_df, phase=\"train\", target_type=[\"city\"])\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = apply_mean(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = stop_mean(train_df, phase=\"train\")\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = epoch_mean(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "\n",
    "# train_df = apply_mean(train_df)\n",
    "# train_df = sort_epoch(train_df)\n",
    "# train_df, score_df = check_score(train_df)\n",
    "# train_df[use_col].to_csv(data_dir / \"baseline_locations_train_with_speed_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(data_dir / \"baseline_locations_test_with_speed.csv\")\n",
    "# use_col = test_df.columns\n",
    "# test_df = linear_interpolation(test_df)\n",
    "# test_df = outlier_correction(test_df)\n",
    "# test_df = apply_kf_smoothing(test_df, phase=\"test\")\n",
    "# test_df = apply_mean(test_df)\n",
    "# test_df = get_removedevice(test_df, 'Pixel4')\n",
    "# test_df = get_removedevice(test_df, 'SamsungS20Ultra')\n",
    "# test_df = get_removedevice(test_df, 'Pixel4XL')\n",
    "# test_df = apply_snap_to_grid(test_df, road_points, min_thr=0, max_thr=100, phase=\"test\", target_type=[\"city\"])\n",
    "# test_df = outlier_interpolate(test_df, phase=\"test\", target_type=[\"city\"])\n",
    "# test_df = stop_mean(test_df, phase=\"test\")\n",
    "# test_df = epoch_mean(test_df)\n",
    "# test_df = apply_mean(test_df)\n",
    "# test_df['phone'] = test_df['collectionName'] + \"_\" + test_df['phoneName']\n",
    "# test_df = sort_epoch(test_df)\n",
    "# test_df[use_col].to_csv(data_dir / \"baseline_locations_test_with_speed_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir / \"baseline_locations_train_with_speed_pp.csv\")\n",
    "test_df = pd.read_csv(data_dir / \"baseline_locations_test_with_speed_pp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sort_epoch(train_df)\n",
    "test_df = sort_epoch(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch:y\n",
    "# yaw:z\n",
    "# roll:x\n",
    "# euler angle -> rotation vector\n",
    "def an2v(y_delta, z_delta, x_delta):\n",
    "    '''\n",
    "    Euler Angles ->Rotation Matrix -> Rotation Vector\n",
    "\n",
    "    Input：\n",
    "        1. y_delta          (float): the angle with rotateing around y-axis.\n",
    "        2. z_delta         (float): the angle with rotateing around z-axis. \n",
    "        3. x_delta         (float): the angle with rotateing around x-axis. \n",
    "    Output：\n",
    "        rx/ry/rz             (float): the rotation vector with rotateing \n",
    "    \n",
    "    Code Ref.: https://www.zacobria.com/universal-robots-knowledge-base-tech-support-forum-hints-tips/python-code-example-of-converting-rpyeuler-angles-to-rotation-vectorangle-axis-for-universal-robots/\n",
    "    (Note：In Code Ref: pitch=y,yaw=z,roll=x. But Google is pitch=x,yaw=z,roll=y)\n",
    "    '''\n",
    "    # yaw: z\n",
    "    Rz_Matrix = np.matrix([\n",
    "    [math.cos(z_delta), -math.sin(z_delta), 0],\n",
    "    [math.sin(z_delta), math.cos(z_delta), 0],\n",
    "    [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # pitch: y\n",
    "    Ry_Matrix = np.matrix([\n",
    "    [math.cos(y_delta), 0, math.sin(y_delta)],\n",
    "    [0, 1, 0],\n",
    "    [-math.sin(y_delta), 0, math.cos(y_delta)]\n",
    "    ])\n",
    "    \n",
    "    # roll: x\n",
    "    Rx_Matrix = np.matrix([\n",
    "    [1, 0, 0],\n",
    "    [0, math.cos(x_delta), -math.sin(x_delta)],\n",
    "    [0, math.sin(x_delta), math.cos(x_delta)]\n",
    "    ])\n",
    "\n",
    "    R = Rz_Matrix * Ry_Matrix * Rx_Matrix\n",
    "\n",
    "    theta = math.acos(((R[0, 0] + R[1, 1] + R[2, 2]) - 1) / 2)\n",
    "    multi = 1 / (2 * math.sin(theta))\n",
    "\n",
    "    rx = multi * (R[2, 1] - R[1, 2]) * theta\n",
    "    ry = multi * (R[0, 2] - R[2, 0]) * theta\n",
    "    rz = multi * (R[1, 0] - R[0, 1]) * theta\n",
    "\n",
    "    return rx, ry, rz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2a(rotation_v):\n",
    "    '''\n",
    "    Rotation Vector -> Rotation Matrix -> Euler Angles\n",
    "\n",
    "    Input：\n",
    "        rx/ry/rz             (float): the rotation vector with rotateing around x/y/z-axis.\n",
    "    Output：\n",
    "        1. y_delta          (float): the angle with rotateing around y-axis.\n",
    "        2. z_delta         (float): the angle with rotateing around z-axis. \n",
    "        3. x_delta         (float): the angle with rotateing around x-axis.  \n",
    "    '''\n",
    "    # Rotation Vector -> Rotation Matrix\n",
    "    R = Rodrigues(rotation_v)[0]\n",
    "\n",
    "    sq = sqrt(R[2,1] ** 2 +  R[2,2] ** 2)\n",
    "\n",
    "    if  not (sq < 1e-6) :\n",
    "        x_delta = atan2(R[2,1] , R[2,2])\n",
    "        y_delta = atan2(-R[2,0], sq)\n",
    "        z_delta = atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x_delta = atan2(-R[1,2], R[1,1])\n",
    "        y_delta = atan2(-R[2,0], sq)\n",
    "        z_delta = 0\n",
    "\n",
    "    return y_delta, z_delta, x_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnss_log_to_dataframes(path):\n",
    "    '''Load GNSS Log'''\n",
    "    print('Loading ' + path, flush = True)\n",
    "    gnss_section_names = {'Raw', 'UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'}\n",
    "    with open(path) as f_open:\n",
    "        datalines = f_open.readlines()\n",
    "\n",
    "    datas = {k: [] for k in gnss_section_names}\n",
    "    gnss_map = {k: [] for k in gnss_section_names}\n",
    "    for dataline in datalines:\n",
    "        is_header = dataline.startswith('#')\n",
    "        dataline = dataline.strip('#').strip().split(',')\n",
    "        # skip over notes, version numbers, etc\n",
    "        if is_header and dataline[0] in gnss_section_names:\n",
    "            gnss_map[dataline[0]] = dataline[1:]\n",
    "        elif not is_header:\n",
    "            datas[dataline[0]].append(dataline[1:])\n",
    "\n",
    "    results = dict()\n",
    "    for k, v in datas.items():\n",
    "        results[k] = pd.DataFrame(v, columns=gnss_map[k])\n",
    "    # pandas doesn't properly infer types from these lists by default\n",
    "    for k, df in results.items():\n",
    "        for col in df.columns:\n",
    "            if col == 'CodeType':\n",
    "                continue\n",
    "            results[k][col] = pd.to_numeric(results[k][col])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imu_data(data_dir, dataset_name, cname, pname, bl_df):\n",
    "    '''Prepare IMU Dataset (For Train: IMU+GT+BL; For Test: IMU+BL)\n",
    "    Input：\n",
    "        1. data_dir: data_dir\n",
    "        2. dataset_name: dataset name（'train'/'test'）\n",
    "        3. cname: CollectionName\n",
    "        4. pname: phoneName\n",
    "        5. bl_df: baseline's dataframe\n",
    "    Output：df_all\n",
    "    '''\n",
    "    # load GNSS log\n",
    "    gnss_df = gnss_log_to_dataframes(str(data_dir / dataset_name / cname / pname / f'{pname}_GnssLog.txt'))\n",
    "    print('sub-dataset shape：')\n",
    "    print('Raw:', gnss_df['Raw'].shape)\n",
    "    print('Status:', gnss_df['Status'].shape)\n",
    "    print('UncalAccel:', gnss_df['UncalAccel'].shape)\n",
    "    print('UncalGyro:', gnss_df['UncalGyro'].shape)\n",
    "    print('UncalMag:', gnss_df['UncalMag'].shape)\n",
    "    print('OrientationDeg:', gnss_df['OrientationDeg'].shape)\n",
    "    print('Fix:', gnss_df['Fix'].shape)\n",
    "\n",
    "    # merge sub-datasets\n",
    "    # accel + gyro\n",
    "    imu_df = pd.merge_asof(gnss_df['UncalAccel'].sort_values('utcTimeMillis'),\n",
    "                           gnss_df['UncalGyro'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "    # (accel + gyro) + mag\n",
    "    imu_df = pd.merge_asof(imu_df.sort_values('utcTimeMillis'),\n",
    "                           gnss_df['UncalMag'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "    # ((accel + gyro) + mag) + OrientationDeg\n",
    "    imu_df = pd.merge_asof(imu_df.sort_values('utcTimeMillis'),\n",
    "                           gnss_df['OrientationDeg'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "   \n",
    "    # UTC->GpsEpoch\n",
    "    imu_df = UTC2GpsEpoch(imu_df)\n",
    "\n",
    "    # print IMU time\n",
    "    dt_offset = pd.to_datetime('1980-01-06 00:00:00')\n",
    "    dt_offset_in_ms = int(dt_offset.value / 1e6)\n",
    "    tmp_datetime = pd.to_datetime(imu_df['millisSinceGpsEpoch'] + dt_offset_in_ms, unit='ms')\n",
    "    print(f\"imu_df time scope: {tmp_datetime.min()} - {tmp_datetime.max()}\")\n",
    "\n",
    "\n",
    "    if dataset_name == 'train':\n",
    "        # read GT dataset\n",
    "        gt_path = data_dir / dataset_name / cname / pname / 'ground_truth.csv'\n",
    "        gt_df = pd.read_csv(gt_path, usecols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg'])\n",
    "        gt_df = augment_data(gt_df)\n",
    "\n",
    "        # print GT time\n",
    "        tmp_datetime = pd.to_datetime(gt_df['millisSinceGpsEpoch'] + dt_offset_in_ms, unit='ms')\n",
    "        print(f\"gt_df time scope: {tmp_datetime.min()} - {tmp_datetime.max()}\")\n",
    "\n",
    "        # merge GT dataset\n",
    "        imu_df = pd.merge_asof(gt_df.sort_values('millisSinceGpsEpoch'),\n",
    "                               imu_df.drop(['elapsedRealtimeNanos'], axis=1).sort_values('millisSinceGpsEpoch'),\n",
    "                               on = 'millisSinceGpsEpoch',\n",
    "                               direction='nearest')\n",
    "    elif dataset_name == 'test':\n",
    "        # merge smaple_df\n",
    "        sample_df = pd.read_csv(data_dir / 'sample_submission.csv')\n",
    "        s_df = sample_df[sample_df[\"phone\"]==f\"{cname}_{pname}\"].reset_index(drop=True)\n",
    "        s_df[\"collectionName\"] = s_df[\"phone\"].str.split(\"_\", expand=True)[0]\n",
    "        s_df[\"phoneName\"] = s_df[\"phone\"].str.split(\"_\", expand=True)[1]\n",
    "        s_df = augment_data(s_df, phase=\"test\")\n",
    "        s_df.drop([\"collectionName\", \"phoneName\"], axis=1, inplace=True)\n",
    "        \n",
    "        imu_df = pd.merge_asof(s_df.sort_values('millisSinceGpsEpoch'),\n",
    "                           imu_df.drop(['elapsedRealtimeNanos'], axis=1).sort_values('millisSinceGpsEpoch'),\n",
    "                           on = 'millisSinceGpsEpoch',\n",
    "                           direction='nearest')\n",
    "\n",
    "    # OrientationDeg -> Rotation Vector\n",
    "    rxs = []\n",
    "    rys = []\n",
    "    rzs = []\n",
    "    for i in range(len(imu_df)):\n",
    "        y_delta = imu_df['rollDeg'].iloc[i]\n",
    "        z_delta = imu_df['yawDeg'].iloc[i]\n",
    "        x_delta = imu_df['pitchDeg'].iloc[i]\n",
    "        rx, ry, rz = an2v(y_delta, z_delta, x_delta)\n",
    "        rxs.append(rx)\n",
    "        rys.append(ry)\n",
    "        rzs.append(rz)\n",
    "\n",
    "    imu_df['ahrsX'] = rxs\n",
    "    imu_df['ahrsY'] = rys\n",
    "    imu_df['ahrsZ'] = rzs\n",
    "\n",
    "    # calibrate sensors' reading\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        imu_df['Accel{}Mps2'.format(axis)] = imu_df['UncalAccel{}Mps2'.format(axis)] - imu_df['Bias{}Mps2'.format(axis)]\n",
    "        imu_df['Gyro{}RadPerSec'.format(axis)] = imu_df['UncalGyro{}RadPerSec'.format(axis)] - imu_df['Drift{}RadPerSec'.format(axis)]\n",
    "        imu_df['Mag{}MicroT'.format(axis)] = imu_df['UncalMag{}MicroT'.format(axis)] - imu_df['Bias{}MicroT'.format(axis)]\n",
    "\n",
    "        # clearn bias features\n",
    "        imu_df.drop([\n",
    "            'Bias{}Mps2'.format(axis), 'Drift{}RadPerSec'.format(axis), 'Bias{}MicroT'.format(axis),\n",
    "            'UncalAccel{}Mps2'.format(axis), 'UncalGyro{}RadPerSec'.format(axis), 'UncalMag{}MicroT'.format(axis)], axis = 1, inplace = True) \n",
    "\n",
    "\n",
    "    if dataset_name == 'train':\n",
    "        # merge Baseline dataset：imu_df + bl_df = (GT + IMU) + Baseline\n",
    "        df_all = pd.merge(imu_df.rename(columns={'latDeg':'target_latDeg', 'lngDeg':'target_lngDeg'}),\n",
    "                      bl_df.drop(['phone'], axis=1).rename(columns={'latDeg':'latDeg_bl','lngDeg':'lngDeg_bl'}),\n",
    "                      on = ['collectionName', 'phoneName', 'millisSinceGpsEpoch'])\n",
    "    elif dataset_name == 'test':\n",
    "        df_all = pd.merge(imu_df,\n",
    "              bl_df[(bl_df['collectionName']==cname) & (bl_df['phoneName']==pname)].drop(['phone'], axis=1).rename(columns={'latDeg':'latDeg_bl','lngDeg':'lngDeg_bl'}),\n",
    "              on = ['millisSinceGpsEpoch'])\n",
    "        df_all.drop(['phone'], axis=1, inplace=True)\n",
    "        \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xyz(df_all, dataset_name):\n",
    "    # baseline: lat/lngDeg -> x/y/z\n",
    "    df_all['Xbl'], df_all['Ybl'], df_all['Zbl'] = zip(*df_all.apply(lambda x: WGS84_to_ECEF(x.latDeg_bl, x.lngDeg_bl, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        # gt: lat/lngDeg -> x/y/z\n",
    "        df_all['Xgt'], df_all['Ygt'], df_all['Zgt'] = zip(*df_all.apply(lambda x: WGS84_to_ECEF(x.target_latDeg, x.target_lngDeg, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "        # copy lat/lngDeg\n",
    "        lat_lng_df = df_all[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'target_latDeg','target_lngDeg', 'latDeg_bl', 'lngDeg_bl']]\n",
    "        df_all.drop(['target_latDeg','target_lngDeg', 'latDeg_bl', 'lngDeg_bl'], axis = 1, inplace = True)\n",
    "    elif dataset_name == 'test':\n",
    "        # copy lat/lngDeg\n",
    "        lat_lng_df = df_all[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg_bl', 'lngDeg_bl']]\n",
    "        df_all.drop(['latDeg_bl', 'lngDeg_bl', 'latDeg','lngDeg',], axis = 1, inplace = True)     \n",
    "        \n",
    "    return lat_lng_df, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WGS84_to_ECEF(lat, lon, alt):\n",
    "    # convert to radians\n",
    "    rad_lat = lat * (np.pi / 180.0)\n",
    "    rad_lon = lon * (np.pi / 180.0)\n",
    "    a    = 6378137.0\n",
    "    # f is the flattening factor\n",
    "    finv = 298.257223563\n",
    "    f = 1 / finv   \n",
    "    # e is the eccentricity\n",
    "    e2 = 1 - (1 - f) * (1 - f)    \n",
    "    # N is the radius of curvature in the prime vertical\n",
    "    N = a / np.sqrt(1 - e2 * np.sin(rad_lat) * np.sin(rad_lat))\n",
    "    x = (N + alt) * np.cos(rad_lat) * np.cos(rad_lon)\n",
    "    y = (N + alt) * np.cos(rad_lat) * np.sin(rad_lon)\n",
    "    z = (N * (1 - e2) + alt)        * np.sin(rad_lat)\n",
    "    return x, y, z\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    {\"proj\":'geocent', \"ellps\":'WGS84', \"datum\":'WGS84'},\n",
    "    {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'},)\n",
    "def ECEF_to_WGS84(x,y,z):\n",
    "    lon, lat, alt = transformer.transform(x,y,z,radians=False)\n",
    "    return lon, lat, alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UTC2GpsEpoch(df):\n",
    "    '''UTC to GpsEpoch\n",
    "    \n",
    "    utcTimeMillis         : UTC epoch (1970/1/1)\n",
    "    millisSinceGpsEpoch   : GPS epoch(1980/1/6 midnight 12:00 UTC)\n",
    "    \n",
    "    Ref: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/discussion/239187\n",
    "    '''\n",
    "    dt_offset = pd.to_datetime('1980-01-06 00:00:00') \n",
    "    dt_offset_in_ms = int(dt_offset.value / 1e6)\n",
    "    df['millisSinceGpsEpoch'] = df['utcTimeMillis'] - dt_offset_in_ms + 18000\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shiftで代替する\n",
    "def create_lag_features(input_df, window_size):\n",
    "    '''prepare training dataset with all aixses'''\n",
    "    # df_all_train = df_all_train.sort_values(\"millisSinceGpsEpoch\").reset_index(drop=True)\n",
    "    output_df = input_df.copy()\n",
    "    \n",
    "    output_df.rename(columns = {'yawDeg':'yawZDeg', 'rollDeg':'rollYDeg', 'pitchDeg':'pitchXDeg'}, inplace = True)\n",
    "    feature_cols = [f for f in list(output_df.columns) if f not in ['Xgt', 'Ygt', 'Zgt', 'collectionName', 'phoneName', 'millisSinceGpsEpoch', 'timeSinceFirstFixSeconds', 'utcTimeMillis', 'elapsedRealtimeNanos']]\n",
    "    for col in feature_cols:\n",
    "        for i in range(1, window_size):\n",
    "            output_df[f\"{col}_{i}\"] = output_df[col].shift(i)\n",
    "        for i in range(1, window_size):\n",
    "            output_df[f\"{col}_{i*-1}\"] = output_df[col].shift(i*-1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_other_axis_feats(df_all, tgt_axis):\n",
    "    '''unrelated-aixs features and uncalibrated features'''\n",
    "    # Clean unrelated-aixs features\n",
    "    all_imu_feats = [\n",
    "        # 'UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2',\n",
    "        #   'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec',\n",
    "        #   'UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT',\n",
    "          'ahrsX', 'ahrsY', 'ahrsZ',\n",
    "          'AccelXMps2', 'AccelYMps2', 'AccelZMps2',\n",
    "          'GyroXRadPerSec', 'GyroZRadPerSec', 'GyroYRadPerSec',\n",
    "          'MagXMicroT', 'MagYMicroT', 'MagZMicroT',\n",
    "          'yawZDeg', 'rollYDeg', 'pitchXDeg',\n",
    "          'Xbl', 'Ybl', 'Zbl'\n",
    "          ]\n",
    "    tgt_imu_feats = []\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        if axis != tgt_axis:\n",
    "            for f in all_imu_feats:\n",
    "                if f.find(axis) >= 0:\n",
    "                    tgt_imu_feats.append(f)\n",
    "            \n",
    "    tmp_drop_feats = []\n",
    "    for f in list(df_all):\n",
    "        if f.split('_')[0] in tgt_imu_feats:\n",
    "            tmp_drop_feats.append(f)\n",
    "\n",
    "    tgt_df = df_all.drop(tmp_drop_feats, axis = 1)\n",
    "    \n",
    "    # Clean uncalibrated features\n",
    "    uncal_feats = [f for f in list(tgt_df) if f.startswith('Uncal') == True]\n",
    "    tgt_df = tgt_df.drop(uncal_feats, axis = 1)\n",
    "    \n",
    "    return tgt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stat_feats(data, tgt_axis):\n",
    "    for f in ['yawZDeg', 'rollYDeg', 'pitchXDeg']:\n",
    "        if f.find(tgt_axis) >= 0:\n",
    "            ori_feat = f\n",
    "            break\n",
    "            \n",
    "    cont_feats = ['heightAboveWgs84EllipsoidM', 'ahrs{}'.format(tgt_axis),\n",
    "           'Accel{}Mps2'.format(tgt_axis), 'Gyro{}RadPerSec'.format(tgt_axis), 'Mag{}MicroT'.format(tgt_axis),\n",
    "            '{}bl'.format(tgt_axis)] + [ori_feat]\n",
    "    \n",
    "    for f in cont_feats:\n",
    "        for w in [1, 2, 4, 8, 16, 30]:\n",
    "            data[f + '_' + str(window_size) + '_mean'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].mean(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_std'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].std(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_max'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].max(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_min'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].min(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_median'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].median(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_skew'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].skew(axis=1)\n",
    "            data[f + '_' + str(window_size) + '_kurt'] = data[[f + f'_{i}' for i in range(-w+1,w) if i!=0]].kurt(axis=1)\n",
    "    return data"
   ]
  },
  {
   "source": [
    "## データの水増し\n",
    "1s間隔なのを線形補間で0.5s間隔にしてみる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n:２の何乗に拡張するか -> 2^n-1\n",
    "# n=2の時は2倍, n=3の時は4倍, n=4の時は8倍\n",
    "def augment_data(input_df, n=2, phase=\"train\", target_collection=[\"city\"]):\n",
    "    logger.info(f\"augment data *{n}\")\n",
    "    logger.info(f\"before:{input_df.shape}\")\n",
    "    dfs = []\n",
    "    for (collectionName, phoneName), df in input_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "        if collection_type_dict[phase][collectionName] in target_collection:\n",
    "            # 前後の時間の平均の時の座標を補完する\n",
    "            df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "            for _ in range(n-1):\n",
    "                # 1s -> 0.5s間隔にする\n",
    "                tmp_df = pd.DataFrame(((df[\"millisSinceGpsEpoch\"] + df[\"millisSinceGpsEpoch\"].shift(1)) / 2).dropna())\n",
    "                tmp_df[\"collectionName\"] = collectionName\n",
    "                tmp_df[\"phoneName\"] = phoneName\n",
    "                tmp_df[\"millisSinceGpsEpoch\"] = tmp_df[\"millisSinceGpsEpoch\"].astype(int)\n",
    "                df = pd.concat([df, tmp_df]).sort_values(\"millisSinceGpsEpoch\")\n",
    "                # 時間で線形補間\n",
    "                df = df.set_index(pd.to_datetime(df[\"millisSinceGpsEpoch\"])).interpolate().reset_index(drop=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    output_df = pd.concat(dfs).reset_index(drop=True)\n",
    "    logger.info(f\"after:{output_df.shape}\")\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "augment data *2\n",
      "before:(131342, 8)\n",
      "after:(154171, 8)\n"
     ]
    }
   ],
   "source": [
    "train_aug_df = augment_data(train_df, phase=\"train\", target_collection=[\"normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "augment data *2\n",
      "before:(91486, 8)\n",
      "after:(124276, 8)\n"
     ]
    }
   ],
   "source": [
    "test_aug_df = augment_data(test_df, phase=\"test\", target_collection=[\"normal\"])"
   ]
  },
  {
   "source": [
    "## preprocessing\n",
    "対象はcityデータのみ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simdkalman\n",
    "\n",
    "def apply_kf_smoothing_for_imu(input_df):\n",
    "    logger.info('[START] Kalman Smoothing for IMU')\n",
    "    # define kf model\n",
    "    target_columns = [\n",
    "        # 'UncalAccelXMps2', 'UncalAccelYMps2','UncalAccelZMps2', \n",
    "        # 'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec','UncalGyroZRadPerSec', \n",
    "        # 'UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT', \n",
    "        'yawDeg', 'rollDeg', 'pitchDeg', \n",
    "        'ahrsX', 'ahrsY', 'ahrsZ', \n",
    "        'AccelXMps2', 'GyroXRadPerSec', 'MagXMicroT', \n",
    "        'AccelYMps2','GyroYRadPerSec', 'MagYMicroT', \n",
    "        'AccelZMps2', 'GyroZRadPerSec', 'MagZMicroT'\n",
    "        ]\n",
    "    df_list = []\n",
    "    for (collection_name, phone_df), df in input_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        for col in target_columns:\n",
    "            T = 1.0\n",
    "            p_noise = 1e-9\n",
    "            o_noise = 1e-9\n",
    "\n",
    "            state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                                        [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "            process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * p_noise\n",
    "            observation_model = np.array([[1, 0, 0, 0, 0, 0]])    \n",
    "            observation_noise = np.diag([5e-5]) + np.ones((1, )) * o_noise\n",
    "            kf = simdkalman.KalmanFilter(\n",
    "                state_transition=state_transition,\n",
    "                process_noise=process_noise,\n",
    "                observation_model=observation_model,\n",
    "                observation_noise=observation_noise)\n",
    "\n",
    "            data = df[col].to_numpy()\n",
    "            data = data.reshape(1, len(data), 1)\n",
    "            smoothed = kf.smooth(data)\n",
    "            df[col] = smoothed.states.mean[0, :, 0]\n",
    "        df_list.append(df)\n",
    "        output_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    return output_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: I use SJC's dataset for training \n",
    "def extract_data_for_ml(input_df, phase=\"train\", target_type=None):\n",
    "    df_list = []\n",
    "    point_list = []\n",
    "    for (collection_name,phone_name), df in tqdm(input_df.groupby(['collectionName', 'phoneName'])):\n",
    "        df = df.sort_values(\"millisSinceGpsEpoch\")\n",
    "        if collection_type_dict[phase][collection_name] in target_type: \n",
    "            try:\n",
    "                df = prepare_imu_data(data_dir, phase, collection_name, phone_name, df)\n",
    "                df = apply_kf_smoothing_for_imu(df)\n",
    "                point_df, df = get_xyz(df, phase)\n",
    "                df = create_lag_features(df, window_size)                    \n",
    "                df_list.append(df)\n",
    "                point_list.append(point_df)\n",
    "            #except:\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "                print(f\"[DROP]{collection_name}, {phone_name}\\n\")\n",
    "     \n",
    "    target_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    target_point_df = pd.concat(point_list).reset_index(drop=True)\n",
    "    return target_df, target_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train, train_point = extract_data_for_ml(train_aug_df, phase=\"train\", target_type=[\"normal\"])\n",
    "# train.to_csv(\"train.csv\", index=False)\n",
    "# train_point.to_csv(\"train_point.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, test_point = extract_data_for_ml(test_aug_df, phase=\"test\", target_type=[\"normal\"])\n",
    "# test.to_csv(\"test.csv\", index=False)\n",
    "# test_point.to_csv(\"test_point.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_dir = \"./\"\n",
    "train = pd.read_csv(referenced_dir + \"train.csv\")\n",
    "train_point = pd.read_csv(referenced_dir + \"train_point.csv\")\n",
    "test = pd.read_csv(referenced_dir + \"test.csv\")\n",
    "test_point = pd.read_csv(referenced_dir + \"test_point.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "co_le = preprocessing.LabelEncoder()\n",
    "ph_le = preprocessing.LabelEncoder()\n",
    "\n",
    "whole = pd.concat([train,test])\n",
    "co_le.fit(whole[\"collectionName\"])\n",
    "ph_le.fit(whole[\"phoneName\"])\n",
    "\n",
    "train[\"le_collectionName\"] = co_le.transform(train[\"collectionName\"])\n",
    "test[\"le_collectionName\"] = co_le.transform(test[\"collectionName\"])\n",
    "train[\"le_phoneName\"] = ph_le.transform(train[\"phoneName\"])\n",
    "test[\"le_phoneName\"] = ph_le.transform(test[\"phoneName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Xgt', 'Ygt', 'Zgt'}"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "set(train.columns) - set(test.columns)"
   ]
  },
  {
   "source": [
    "## features DEA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    # 目的関数. これの意味で最小となるようなパラメータを探します. \n",
    "    'objective': 'mse', \n",
    "    'metric':'mse',\n",
    "\n",
    "     # 学習率. 小さいほどなめらかな決定境界が作られて性能向上に繋がる場合が多いです、\n",
    "    # がそれだけ木を作るため学習に時間がかかります\n",
    "    'learning_rate': .01,\n",
    "\n",
    "    # L2 Reguralization\n",
    "    'reg_lambda': 1.,\n",
    "    # こちらは L1 \n",
    "    'reg_alpha': .1,\n",
    "\n",
    "    # 木の深さ. 深い木を許容するほどより複雑な交互作用を考慮するようになります\n",
    "    'max_depth': 5, \n",
    "\n",
    "    # 木の最大数. early_stopping という枠組みで木の数は制御されるようにしていますのでとても大きい値を指定しておきます.\n",
    "    'n_estimators': 10000, \n",
    "\n",
    "    # 木を作る際に考慮する特徴量の割合. 1以下を指定すると特徴をランダムに欠落させます。小さくすることで, まんべんなく特徴を使うという効果があります.\n",
    "    'colsample_bytree': .5,\n",
    "\n",
    "    # 最小分割でのデータ数. 小さいとより細かい粒度の分割方法を許容します.\n",
    "    'min_child_samples': 10,\n",
    "\n",
    "    # bagging の頻度と割合\n",
    "    'subsample_freq': 3,\n",
    "    'subsample': .9,\n",
    "\n",
    "    # 特徴重要度計算のロジック(後述)\n",
    "    'importance_type': 'gain', \n",
    "    'random_state': 71,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from wandb.lightgbm import wandb_callback\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "import wandb\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "def fit_lgbm(train_df,\n",
    "             test_df,\n",
    "             target_axis=None,\n",
    "             params: dict=None, \n",
    "             pretrained_exp=None,\n",
    "             verbose: int=50\n",
    "             ):\n",
    "\n",
    "    train_df = remove_other_axis_feats(train_df, target_axis)\n",
    "    train_df = add_stat_feats(train_df, target_axis)\n",
    "    test_df = remove_other_axis_feats(test_df, target_axis)\n",
    "    test_df = add_stat_feats(test_df, target_axis)\n",
    "\n",
    "    feature_names = [f for f in list(train_df) if f not in ['Xgt', 'Ygt', 'Zgt', 'collectionName', 'phoneName', 'millisSinceGpsEpoch', 'is_train', 'phone', 'place']]\n",
    "    target = '{}gt'.format(target_axis)\n",
    "\n",
    "    # パラメータがないときは、空の dict で置き換える\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    oofs = []  # 全てのoofをdfで格納する\n",
    "    preds = []  # 全ての予測値をdfで格納する\n",
    "    val_scores = []\n",
    "    # skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    n_splits = train[\"collectionName\"].nunique()\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    train_fold = [(trn_idx, val_idx) for trn_idx, val_idx in gkf.split(train_df, groups=train_df['collectionName'])]\n",
    "    for fold in range(n_splits):\n",
    "        # 指定したfoldのみループを回す\n",
    "        # if fold not in USE_FOLDS:\n",
    "        #     continue\n",
    "\n",
    "        print('=' * 20)\n",
    "        print(f'Fold {fold}')\n",
    "        print('=' * 20)\n",
    "\n",
    "\n",
    "        # training data の target と同じだけのゼロ配列を用意\n",
    "        oof_pred = np.zeros(train_df.shape[0], dtype=np.float)\n",
    "\n",
    "        # train/valid data\n",
    "        trn_idx_for_train, val_idx_for_train = train_fold[fold]\n",
    "        x_train = train_df.loc[trn_idx_for_train, feature_names]  # .reset_index(drop=True)\n",
    "        x_valid = train_df.loc[val_idx_for_train, feature_names]  # .reset_index(drop=True)\n",
    "        y_train = train_df.loc[trn_idx_for_train, target]  # .reset_index(drop=True)\n",
    "        y_valid = train_df.loc[val_idx_for_train, target]  # .reset_index(drop=True)\n",
    "\n",
    "        if pretrained_exp is None:\n",
    "            model_dir = f\"../../model/{EXP_NAME}/\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "        else:\n",
    "            model_dir = f\"../../model/{pretrained_exp}/\"\n",
    "        # logger\n",
    "        wandb.init(project='outdoor', entity='kuto5046', group=EXP_NAME)\n",
    "        wandb.run.name = EXP_NAME + f'-fold-{fold}'\n",
    "        wandb_config = wandb.config\n",
    "        wandb_config['model_name'] = \"lightGBM\"\n",
    "        \n",
    "        if os.path.exists(model_dir + f'{target_axis}_fold{fold}.pkl'):\n",
    "            clf = pickle.load(open(model_dir + f'{target_axis}_fold{fold}.pkl', 'rb'))\n",
    "        else:\n",
    "            clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(x_train, y_train,\n",
    "                    eval_set=[(x_valid, y_valid)],  \n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose=verbose,\n",
    "                    callbacks=[wandb_callback()])\n",
    "            pickle.dump(clf, open(model_dir + f'{target_axis}_fold{fold}.pkl', 'wb'))\n",
    "\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        x_valid[f\"{target_axis}pred\"] = pred_i\n",
    "        x_valid[target] = y_valid.to_numpy()\n",
    "                \n",
    "        x_valid[\"collectionName\"] = train.loc[val_idx_for_train, \"collectionName\"].values\n",
    "        x_valid[\"phoneName\"] = train.loc[val_idx_for_train, \"phoneName\"].values\n",
    "\n",
    "        oofs.append(x_valid)\n",
    "        \n",
    "        score = mean_squared_error(y_valid, pred_i)\n",
    "        val_scores.append(score)\n",
    "        print(f'Fold {fold} MSE: {score:.4f}')\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        pred = clf.predict(test_df[feature_names])\n",
    "        preds.append(pred)\n",
    "    oof_df = pd.concat(oofs).sort_index() \n",
    "    all_score = mean_squared_error(oof_df[target], oof_df[f\"{target_axis}pred\"])\n",
    "    print('-' * 50)\n",
    "    print('FINISHED | Whole MSE: {:.4f}'.format(all_score))\n",
    "    features = x_train.columns.values\n",
    "    return oof_df, np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"phone\"] = train[\"collectionName\"] + \"_\" + train[\"phoneName\"]\n",
    "train[\"place\"] = train[\"collectionName\"].str[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "====================\n",
      "Fold 0\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkuto5046\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">glamorous-rain-1362</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/outdoor\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/1u5k8gvz\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/1u5k8gvz</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp078/wandb/run-20210719_234709-1u5k8gvz</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 1.17718e+06\n",
      "[200]\tvalid_0's l2: 1.07904e+06\n",
      "[300]\tvalid_0's l2: 1.04017e+06\n",
      "Early stopping, best iteration is:\n",
      "[299]\tvalid_0's l2: 1.03936e+06\n",
      "Fold 0 MSE: 1039368.5038\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2601059<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3094d54f66dd450784f38aa12e4a20d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234709-1u5k8gvz/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234709-1u5k8gvz/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>1040663.04091</td></tr><tr><td>_runtime</td><td>11</td></tr><tr><td>_timestamp</td><td>1626706041</td></tr><tr><td>_step</td><td>348</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>█▆▅▅▆▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>_timestamp</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">glamorous-rain-1362</strong>: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/1u5k8gvz\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/1u5k8gvz</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">misty-snowflake-1363</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/outdoor\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/1kmia9p7\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/1kmia9p7</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp078/wandb/run-20210719_234725-1kmia9p7</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 290761\n",
      "[200]\tvalid_0's l2: 39104.1\n",
      "[300]\tvalid_0's l2: 5306.69\n",
      "[400]\tvalid_0's l2: 743.298\n",
      "[500]\tvalid_0's l2: 115.838\n",
      "[600]\tvalid_0's l2: 25.9173\n",
      "[700]\tvalid_0's l2: 12.7828\n",
      "[800]\tvalid_0's l2: 10.4244\n",
      "[900]\tvalid_0's l2: 9.89943\n",
      "[1000]\tvalid_0's l2: 9.76473\n",
      "[1100]\tvalid_0's l2: 9.69327\n",
      "Early stopping, best iteration is:\n",
      "[1123]\tvalid_0's l2: 9.68496\n",
      "Fold 1 MSE: 9.6713\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2601171<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3319b19c691458581fe12e285da7032"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234725-1kmia9p7/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234725-1kmia9p7/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>9.75566</td></tr><tr><td>_runtime</td><td>19</td></tr><tr><td>_timestamp</td><td>1626706064</td></tr><tr><td>_step</td><td>1172</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">misty-snowflake-1363</strong>: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/1kmia9p7\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/1kmia9p7</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 2\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">summer-water-1364</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/outdoor\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/39pz3f3d\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/39pz3f3d</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp078/wandb/run-20210719_234747-39pz3f3d</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 275345\n",
      "[200]\tvalid_0's l2: 37131.1\n",
      "[300]\tvalid_0's l2: 4950.91\n",
      "[400]\tvalid_0's l2: 657.817\n",
      "[500]\tvalid_0's l2: 110.38\n",
      "[600]\tvalid_0's l2: 51.7197\n",
      "[700]\tvalid_0's l2: 47.3787\n",
      "[800]\tvalid_0's l2: 46.5136\n",
      "Early stopping, best iteration is:\n",
      "[761]\tvalid_0's l2: 45.9671\n",
      "Fold 2 MSE: 45.9509\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2601278<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52ad106967944d569002854dfa0dd8d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234747-39pz3f3d/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234747-39pz3f3d/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>46.97029</td></tr><tr><td>_runtime</td><td>16</td></tr><tr><td>_timestamp</td><td>1626706083</td></tr><tr><td>_step</td><td>810</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">summer-water-1364</strong>: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/39pz3f3d\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/39pz3f3d</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 3\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">autumn-morning-1365</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/outdoor\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/3q851opm\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/3q851opm</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp078/wandb/run-20210719_234806-3q851opm</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 2.04237e+06\n",
      "[200]\tvalid_0's l2: 336310\n",
      "[300]\tvalid_0's l2: 83098.3\n",
      "[400]\tvalid_0's l2: 40570.9\n",
      "[500]\tvalid_0's l2: 31472.7\n",
      "[600]\tvalid_0's l2: 29316.4\n",
      "[700]\tvalid_0's l2: 28709.2\n",
      "[800]\tvalid_0's l2: 28411.5\n",
      "[900]\tvalid_0's l2: 28349.1\n",
      "[1000]\tvalid_0's l2: 28322.2\n",
      "[1100]\tvalid_0's l2: 28299.2\n",
      "[1200]\tvalid_0's l2: 28292.2\n",
      "Early stopping, best iteration is:\n",
      "[1184]\tvalid_0's l2: 28290\n",
      "Fold 3 MSE: 28289.9440\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2601373<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "334b4c43b9ec4daf88e542f38846df3e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234806-3q851opm/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/user/work/exp/exp078/wandb/run-20210719_234806-3q851opm/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>28290.89847</td></tr><tr><td>_runtime</td><td>17</td></tr><tr><td>_timestamp</td><td>1626706103</td></tr><tr><td>_step</td><td>1233</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>valid_0_l2</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">autumn-morning-1365</strong>: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/3q851opm\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/3q851opm</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 4\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">fragrant-dragon-1366</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/outdoor\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/outdoor/runs/2m2qp83d\" target=\"_blank\">https://wandb.ai/kuto5046/outdoor/runs/2m2qp83d</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp078/wandb/run-20210719_234827-2m2qp83d</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-414890f8a53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pretrained_exp = \"exp066\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpretrained_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mXoof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mYoof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mZoof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-44c8638881b3>\u001b[0m in \u001b[0;36mfit_lgbm\u001b[0;34m(train_df, test_df, target_axis, params, pretrained_exp, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             clf.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    768\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    769\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    771\u001b[0m                                        \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                                        \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pretrained_exp = \"exp066\"\n",
    "pretrained_exp = None\n",
    "Xoof_df, Xpred = fit_lgbm(train, test, \"X\", params, pretrained_exp, verbose=100)\n",
    "Yoof_df, Ypred = fit_lgbm(train, test, \"Y\", params, pretrained_exp, verbose=100)\n",
    "Zoof_df, Zpred = fit_lgbm(train, test, \"Z\", params, pretrained_exp, verbose=100)"
   ]
  },
  {
   "source": [
    "X: Whole MSE: 33.6639  \n",
    "Y: Whole MSE: 35.5370   \n",
    "Z: Whole MSE: 33.2243   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoof_df[['Xgt', 'Xpred']].plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoof_df[['Ygt', 'Ypred']].plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoof_df[['Zgt', 'Zpred']].plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Pseudo Labeling\n",
    "test[\"Xgt\"] = Xpred\n",
    "test[\"Ygt\"] = Ypred\n",
    "test[\"Zgt\"] = Zpred\n",
    "test.to_csv(\"test_pl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xyz -> lng, lat\n",
    "lng_gt, lat_gt, _ = ECEF_to_WGS84(Xoof_df['Xgt'].values, Yoof_df['Ygt'].values, Zoof_df['Zgt'].values)\n",
    "lng_pred, lat_pred, _ = ECEF_to_WGS84(Xoof_df['Xpred'].values,Yoof_df['Ypred'].values,Zoof_df['Zpred'].values)\n",
    "lng_test_pred, lat_test_pred, _ = ECEF_to_WGS84(Xpred, Ypred, Zpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng_gt.shape, lng_pred.shape, lng_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"collectionName\", \"phoneName\"]]\n",
    "train['target_latDeg'] = lat_gt\n",
    "train['target_lngDeg'] = lng_gt\n",
    "train['latDeg'] = lat_pred\n",
    "train['lngDeg'] = lng_pred\n",
    "\n",
    "test = test[[\"collectionName\", \"phoneName\"]]\n",
    "test[\"latDeg\"] = lat_test_pred\n",
    "test[\"lngDeg\"] = lng_test_pred"
   ]
  },
  {
   "source": [
    "データ点が増えスコアが正しく評価できないのでここは飛ばす"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(test,                              \n",
    "                        # Here, plotly gets, (x,y) coordinates\n",
    "                        lat=\"latDeg\",\n",
    "                        lon=\"lngDeg\",\n",
    "                        # text='elapsed_time',\n",
    "                            \n",
    "                        #Here, plotly detects color of series\n",
    "                        color=\"collectionName\",\n",
    "                        labels=\"collectionName\",\n",
    "                            \n",
    "                        zoom=9,\n",
    "                        center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                        height=600,\n",
    "                        width=800)\n",
    "fig.update_layout(mapbox_style='stamen-terrain')\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# fig.update_layout(title_text=\"{collection_name}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_df = train_aug_df.copy()\n",
    "_train_df[\"latDeg2\"] = np.nan\n",
    "_train_df[\"lngDeg2\"] = np.nan\n",
    "for (collection_name, phone_name), df in train.groupby([\"collectionName\", \"phoneName\"]):\n",
    "    break\n",
    "query = (_train_df['collectionName']==collection_name)&(_train_df[\"phoneName\"]==phone_name)\n",
    "_train_df.loc[query, \"latDeg2\"] = df[\"latDeg\"].values\n",
    "_train_df.loc[query, \"lngDeg2\"] = df[\"lngDeg\"].values\n",
    "_train_df.dropna(inplace=True)\n",
    "df1 = _train_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\", \"latDeg\", \"lngDeg\"]]\n",
    "df1[\"type\"] = \"baseline\"\n",
    "df2 = _train_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\", \"latDeg2\", \"lngDeg2\"]].rename(columns={\"latDeg2\":\"latDeg\", \"lngDeg2\":\"lngDeg\"})\n",
    "df2[\"type\"] = \"imu\"\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df,                              \n",
    "                        # Here, plotly gets, (x,y) coordinates\n",
    "                        lat=\"latDeg\",\n",
    "                        lon=\"lngDeg\",\n",
    "                        # text='elapsed_time',\n",
    "                            \n",
    "                        #Here, plotly detects color of series\n",
    "                        color=\"type\",\n",
    "                        labels=\"type\",\n",
    "                            \n",
    "                        zoom=9,\n",
    "                        center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                        height=600,\n",
    "                        width=800)\n",
    "fig.update_layout(mapbox_style='stamen-terrain')\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# fig.update_layout(title_text=\"{collection_name}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_df = test_aug_df.copy()\n",
    "_test_df[\"latDeg2\"] = np.nan\n",
    "_test_df[\"lngDeg2\"] = np.nan\n",
    "for (collection_name, phone_name), df in test.groupby([\"collectionName\", \"phoneName\"]):\n",
    "    if collection_name == \"2021-04-29-US-SJC-3\":\n",
    "        break\n",
    "    \n",
    "print(collection_name)\n",
    "query = (_test_df['collectionName']==collection_name)&(_test_df[\"phoneName\"]==phone_name)\n",
    "_test_df.loc[query, \"latDeg2\"] = df[\"latDeg\"].values\n",
    "_test_df.loc[query, \"lngDeg2\"] = df[\"lngDeg\"].values\n",
    "_test_df.dropna(inplace=True)\n",
    "df1 = _test_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\", \"latDeg\", \"lngDeg\"]]\n",
    "df1[\"type\"] = \"baseline\"\n",
    "\n",
    "df2 = _test_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\", \"latDeg2\", \"lngDeg2\"]].rename(columns={\"latDeg2\":\"latDeg\", \"lngDeg2\":\"lngDeg\"})\n",
    "df2[\"type\"] = \"imu\"\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df,                              \n",
    "                        # Here, plotly gets, (x,y) coordinates\n",
    "                        lat=\"latDeg\",\n",
    "                        lon=\"lngDeg\",\n",
    "                        # text='elapsed_time',\n",
    "                            \n",
    "                        #Here, plotly detects color of series\n",
    "                        color=\"type\",\n",
    "                        labels=\"type\",\n",
    "                            \n",
    "                        zoom=9,\n",
    "                        center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                        height=600,\n",
    "                        width=800)\n",
    "fig.update_layout(mapbox_style='stamen-terrain')\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# fig.update_layout(title_text=\"{collection_name}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, _ = check_score(train_df)  # before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_df = train_aug_df.copy()\n",
    "# _train_df = train_df.copy()\n",
    "for (collection_name, phone_name), df in train.groupby([\"collectionName\", \"phoneName\"]):\n",
    "    query = (_train_df['collectionName']==collection_name)&(_train_df[\"phoneName\"]==phone_name)\n",
    "    _train_df.loc[query, \"latDeg\"] = df[\"latDeg\"].values\n",
    "    _train_df.loc[query, \"lngDeg\"] = df[\"lngDeg\"].values\n",
    "    # _train_df.loc[query, \"latDeg\"] = (_train_df.loc[query, \"latDeg\"].values + df[\"latDeg\"].values)/2  # 平均をとる\n",
    "    # _train_df.loc[query, \"lngDeg\"] = (_train_df.loc[query, \"lngDeg\"].values + df[\"lngDeg\"].values)/2\n",
    "\n",
    "_train_df = train_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"]].merge(_train_df, on=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_df, _ = check_score(_train_df)  # after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_df = test_aug_df.copy()\n",
    "for (collection_name, phone_name), df in test.groupby([\"collectionName\", \"phoneName\"]):\n",
    "    query = (_test_df['collectionName']==collection_name)&(_test_df[\"phoneName\"]==phone_name)\n",
    "    if query.sum() == len(df):\n",
    "        _test_df.loc[query, \"latDeg\"] = df[\"latDeg\"].values\n",
    "        _test_df.loc[query, \"lngDeg\"] = df[\"lngDeg\"].values\n",
    "        # _test_df.loc[query, \"latDeg\"] = (_test_df.loc[query, \"latDeg\"] + df[\"latDeg\"].values)/2\n",
    "        # _test_df.loc[query, \"lngDeg\"] = (_test_df.loc[query, \"lngDeg\"] + df[\"lngDeg\"].values)/2\n",
    "    else:\n",
    "        print(collection_name)\n",
    "        print(phone_name)\n",
    "\n",
    "_test_df = test_df[[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"]].merge(_test_df, on=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_df.to_csv(\"delta_train.csv\", index=False)\n",
    "_test_df.to_csv(\"delta_test.csv\", index=False)"
   ]
  },
  {
   "source": [
    "### ML後にppを実施"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_dir = \"./\"\n",
    "train_df = pd.read_csv(referenced_dir + \"delta_train.csv\")\n",
    "test_df = pd.read_csv(referenced_dir + \"delta_test.csv\")"
   ]
  },
  {
   "source": [
    "### 局所的なスコアの伸びを把握したい。↓"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = [k for k,v in collection_type_dict[\"train\"].items() if v==\"city\"]\n",
    "t_df = train_df[train_df[\"collectionName\"].isin(train_collection)]\n",
    "\n",
    "t_df = apply_kf_smoothing(t_df, phase=\"train\")\n",
    "t_df, score_df = check_score(t_df)\n",
    "\n",
    "t_df = apply_snap_to_grid(t_df, road_points, min_thr=0, max_thr=100, phase=\"train\")\n",
    "t_df, score_df = check_score(t_df)\n",
    "\n",
    "t_df = outlier_interpolate(t_df, thr=0.005)\n",
    "t_df, score_df = check_score(t_df)\n",
    "\n",
    "t_df = stop_mean(t_df, phase=\"train\")\n",
    "t_df, score_df = check_score(t_df)\n",
    "\n",
    "t_df = apply_mean(t_df)\n",
    "t_df, score_df = check_score(t_df)\n",
    "\n",
    "# t_df = apply_kf_smoothing(t_df, phase=\"train\")\n",
    "# t_df, score_df = check_score(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.sort_values(\"mean_score\", ascending=False).head(10).style.bar(subset=['p_50_score', 'p_95_score', 'mean_score'], color=['teal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_mapbox(t_df,                              \n",
    "#                         # Here, plotly gets, (x,y) coordinates\n",
    "#                         lat=\"latDeg\",\n",
    "#                         lon=\"lngDeg\",\n",
    "#                         # text='elapsed_time',\n",
    "                            \n",
    "#                         #Here, plotly detects color of series\n",
    "#                         color=\"collectionName\",\n",
    "#                         labels=\"collectionName\",\n",
    "                            \n",
    "#                         zoom=9,\n",
    "#                         center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "#                         height=600,\n",
    "#                         width=800)\n",
    "# fig.update_layout(mapbox_style='stamen-terrain')\n",
    "# fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# # fig.update_layout(title_text=\"{collection_name}\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, score_df = check_score(train_df)\n",
    "\n",
    "train_df = apply_kf_smoothing(train_df, phase=\"train\", target_type=[\"city\"])\n",
    "train_df, score_df = check_score(train_df)\n",
    "\n",
    "train_df = apply_snap_to_grid(train_df, road_points, min_thr=0, max_thr=100, phase=\"train\", target_type=[\"city\"])\n",
    "train_df, score_df = check_score(train_df)\n",
    "\n",
    "train_df = outlier_interpolate(train_df, thr=0.005, target_type=[\"city\"])\n",
    "train_df, score_df = check_score(train_df)\n",
    "\n",
    "train_df = stop_mean(train_df, phase=\"train\")\n",
    "train_df, score_df = check_score(train_df)\n",
    "\n",
    "train_df = apply_mean(train_df)\n",
    "train_df, score_df = check_score(train_df)\n",
    "train_df.to_csv(\"oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = apply_kf_smoothing(test_df, phase=\"test\", target_type=[\"city\"])\n",
    "test_df = apply_snap_to_grid(test_df, road_points, min_thr=0, max_thr=100, phase=\"test\", target_type=[\"city\"])\n",
    "test_df = outlier_interpolate(test_df, phase=\"test\", thr=0.005, target_type=[\"city\"])\n",
    "test_df = stop_mean(test_df, phase=\"test\")\n",
    "test_df = apply_mean(test_df)\n",
    "test_df['phone'] = test_df['collectionName'] + \"_\" + test_df['phoneName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (collection_name, phone_name), df in train_df.groupby([\"collectionName\", \"phoneName\"]):\n",
    "#     if collection_type_dict[\"train\"][collection_name] == \"city\":\n",
    "\n",
    "#         df = df.sort_values(\"millisSinceGpsEpoch\").reset_index(drop=True)\n",
    "#         df[\"dist\"] = calc_haversine(df[\"latDeg\"],df[\"lngDeg\"],df[\"latDeg\"].shift(1),df[\"lngDeg\"].shift(1)).values\n",
    "#         df[\"delta_t\"] = df[\"millisSinceGpsEpoch\"] - df[\"millisSinceGpsEpoch\"].shift(1)\n",
    "#         df[\"speed\"] = df[\"dist\"] / df[\"delta_t\"]\n",
    "#         df[\"speed_pre_delta\"] = (df[\"speed\"] - df[\"speed\"].shift(1)).abs()\n",
    "#         df[\"speed_post_delta\"] = (df[\"speed\"] - df[\"speed\"].shift(-1)).abs()\n",
    "#         # df[\"speed\"].plot(figsize=(20,5))\n",
    "#         df[\"speed_post_delta\"].plot(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_df = pd.read_csv(data_dir / 'sample_submission.csv')\n",
    "sub_df = sub_df.drop(['latDeg', 'lngDeg'], axis=1).merge(test_df[['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=['phone', 'millisSinceGpsEpoch'])\n",
    "sub_df.to_csv(f'{EXP_NAME}_submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df2 = pd.DataFrame()\n",
    "for phone, df in test_df.groupby(\"phone\"):\n",
    "    df[\"elapsed_time\"] = (df[\"millisSinceGpsEpoch\"] - df[\"millisSinceGpsEpoch\"].min()) / 1000\n",
    "    test_df2 = pd.concat([test_df2, df]).reset_index(drop=True)\n",
    "test_df2[\"elapsed_time\"] = test_df2[\"elapsed_time\"].astype(str)\n",
    "\n",
    "fig = px.scatter_mapbox(test_df2,                              \n",
    "                        # Here, plotly gets, (x,y) coordinates\n",
    "                        lat=\"latDeg\",\n",
    "                        lon=\"lngDeg\",\n",
    "                        text='elapsed_time',\n",
    "                            \n",
    "                        #Here, plotly detects color of series\n",
    "                        color=\"collectionName\",\n",
    "                        labels=\"collectionName\",\n",
    "                        hover_data=[\"elapsed_time\", \"phoneName\"],\n",
    "                            \n",
    "                        zoom=9,\n",
    "                        center={\"lat\":37.423576, \"lon\":-122.094132},\n",
    "                        height=600,\n",
    "                        width=800)\n",
    "fig.update_layout(mapbox_style='stamen-terrain')\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# fig.update_layout(title_text=\"{collection_name}\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_sub_df = pd.read_csv(f'../exp069/exp069_submission.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "calc_haversine(sub_df['latDeg'],sub_df['lngDeg'],_sub_df['latDeg'], _sub_df['lngDeg']).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00bf5914059f494290386ac1c56c3384": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "068a8ef1559b44d4a38630746583cdb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0912bc8b3d75422daa956ef375f20973": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "123dcd5d03694aac81a370d105d714c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ac0922e712d14f06b538b7cd7952a583",
       "max": 73,
       "style": "IPY_MODEL_d4f323ff72c548569b7472e06aeba22c",
       "value": 73
      }
     },
     "1731c5d17bde45f480aebc541b264685": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "28995f4b8499485c8c1c4411182eb8b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2cf2702fc2bd45feb0513e286cbdbddb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b3e8f41a248a45f6b30ac8d9ffb4ea88",
       "style": "IPY_MODEL_8a6ad24c04344260bb10e8e00c9f5b19",
       "value": "100%"
      }
     },
     "43c86d09a3b14ef28d599fe69bbba767": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "467720a431004ad4a3996d8e47aa536e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d643960b74584b9582fb39e98b3bafd4",
       "style": "IPY_MODEL_068a8ef1559b44d4a38630746583cdb6",
       "value": " 48/48 [00:11&lt;00:00,  4.10it/s]"
      }
     },
     "53a41e5c01ea4a85bd669986a884b774": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dce007f50e674165ade4c4b6cc0ba49d",
       "style": "IPY_MODEL_ca439ceb54c74bb09b694d02c7aebef1",
       "value": " 73/73 [00:16&lt;00:00,  3.81it/s]"
      }
     },
     "585620b1f7624183aef31ed111e6151c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1731c5d17bde45f480aebc541b264685",
       "style": "IPY_MODEL_28995f4b8499485c8c1c4411182eb8b1",
       "value": " 73/73 [00:22&lt;00:00,  3.04it/s]"
      }
     },
     "748f3b580b674da1b5961c305af8a363": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a6ac9555bdb04d6ba08a7df7b33ea1cd",
       "style": "IPY_MODEL_b715677afe9043e081bbb6c955442857",
       "value": "100%"
      }
     },
     "7f7218df56a640c49489de28b11ac81b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8a3ef9f30f4847da931b2260b7a1612f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c876913e7c274f4b8572293ba06a0c64",
        "IPY_MODEL_c45d78a7bd4d4e228d01d07b8a875ff7",
        "IPY_MODEL_467720a431004ad4a3996d8e47aa536e"
       ],
       "layout": "IPY_MODEL_00bf5914059f494290386ac1c56c3384"
      }
     },
     "8a6ad24c04344260bb10e8e00c9f5b19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "940c0aceec324198b44286e2f5f628a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "94eb7f70e0c445b8a1c1c043576401e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2cf2702fc2bd45feb0513e286cbdbddb",
        "IPY_MODEL_123dcd5d03694aac81a370d105d714c1",
        "IPY_MODEL_53a41e5c01ea4a85bd669986a884b774"
       ],
       "layout": "IPY_MODEL_0912bc8b3d75422daa956ef375f20973"
      }
     },
     "a6ac9555bdb04d6ba08a7df7b33ea1cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6d2268fffd648988dcb13046734b2b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a8fd4ff1307f4e42903c052b2875e263": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_748f3b580b674da1b5961c305af8a363",
        "IPY_MODEL_c482b7a80a2b47b49dd4ea1424d14d28",
        "IPY_MODEL_585620b1f7624183aef31ed111e6151c"
       ],
       "layout": "IPY_MODEL_c8c52c02fa12439d8b46926dbb3a01fe"
      }
     },
     "ac0922e712d14f06b538b7cd7952a583": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3e8f41a248a45f6b30ac8d9ffb4ea88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5bbd06af5994a759d7e8a24c4f9249c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b715677afe9043e081bbb6c955442857": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c45d78a7bd4d4e228d01d07b8a875ff7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7f7218df56a640c49489de28b11ac81b",
       "max": 48,
       "style": "IPY_MODEL_940c0aceec324198b44286e2f5f628a8",
       "value": 48
      }
     },
     "c482b7a80a2b47b49dd4ea1424d14d28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c5e0bcff216f417ab0efb39ef7ad34f1",
       "max": 73,
       "style": "IPY_MODEL_b5bbd06af5994a759d7e8a24c4f9249c",
       "value": 73
      }
     },
     "c5e0bcff216f417ab0efb39ef7ad34f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c876913e7c274f4b8572293ba06a0c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a6d2268fffd648988dcb13046734b2b5",
       "style": "IPY_MODEL_43c86d09a3b14ef28d599fe69bbba767",
       "value": "100%"
      }
     },
     "c8c52c02fa12439d8b46926dbb3a01fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca439ceb54c74bb09b694d02c7aebef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d4f323ff72c548569b7472e06aeba22c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d643960b74584b9582fb39e98b3bafd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dce007f50e674165ade4c4b6cc0ba49d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}